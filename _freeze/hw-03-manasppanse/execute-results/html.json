{
  "hash": "2ea16a2a230059eb42460961a107bc0e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Classification & Model Evaluation\"\nauthor:\n  - name: \"Manas P Panse\"\n    affiliation: \"College of Information Science, University of Arizona\"\nformat:\n   html:\n    code-tools: true\n    code-overflow: wrap\n    embed-resources: true\ncode-annotations: hover\nexecute:\n  warning: false\n  messae: false\n  error: false\ntoc: true\n---\n\n# 0 - Pre-Checks\n\n::: {#python-version .cell execution_count=1}\n``` {.python .cell-code}\n# Checking Python Version\n\n!python --version\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPython 3.12.3\n```\n:::\n:::\n\n\n::: {#import-libraries .cell execution_count=2}\n``` {.python .cell-code}\n# Importing Necessary Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score, make_scorer\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score\nfrom sklearn.model_selection import cross_validate, cross_val_score, StratifiedKFold, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n```\n:::\n\n\n::: {#import-dataset .cell execution_count=3}\n``` {.python .cell-code}\n# Importing Dataset\n\ncosts_df = pd.read_csv(\"data/hw-03/childcare_costs.csv\")\n```\n:::\n\n\n# 1 - Data Preparation and Exploration\n\n## Task 1 - Exploratory Data Analysis (1 Cr.)\n\n### Data Overview\n\n::: {#eda .cell execution_count=4}\n``` {.python .cell-code}\ncosts_df.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 23342 entries, 0 to 23341\nData columns (total 64 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   county_fips_code           23342 non-null  int64  \n 1   study_year                 23342 non-null  int64  \n 2   unr_16                     23342 non-null  float64\n 3   funr_16                    23342 non-null  float64\n 4   munr_16                    23342 non-null  float64\n 5   unr_20to64                 23342 non-null  float64\n 6   funr_20to64                23342 non-null  float64\n 7   munr_20to64                23342 non-null  float64\n 8   flfpr_20to64               23342 non-null  float64\n 9   flfpr_20to64_under6        23342 non-null  float64\n 10  flfpr_20to64_6to17         23342 non-null  float64\n 11  flfpr_20to64_under6_6to17  23342 non-null  float64\n 12  mlfpr_20to64               23342 non-null  float64\n 13  pr_f                       23342 non-null  float64\n 14  pr_p                       23342 non-null  float64\n 15  mhi_2018                   23342 non-null  float64\n 16  me_2018                    23342 non-null  float64\n 17  fme_2018                   23342 non-null  float64\n 18  mme_2018                   23342 non-null  float64\n 19  total_pop                  23342 non-null  int64  \n 20  one_race                   23342 non-null  float64\n 21  one_race_w                 23342 non-null  float64\n 22  one_race_b                 23342 non-null  float64\n 23  one_race_i                 23342 non-null  float64\n 24  one_race_a                 23342 non-null  float64\n 25  one_race_h                 23342 non-null  float64\n 26  one_race_other             23342 non-null  float64\n 27  two_races                  23342 non-null  float64\n 28  hispanic                   23342 non-null  float64\n 29  households                 23342 non-null  int64  \n 30  h_under6_both_work         23342 non-null  int64  \n 31  h_under6_f_work            23342 non-null  int64  \n 32  h_under6_m_work            23342 non-null  int64  \n 33  h_under6_single_m          23342 non-null  float64\n 34  h_6to17_both_work          23342 non-null  int64  \n 35  h_6to17_fwork              23342 non-null  int64  \n 36  h_6to17_mwork              23342 non-null  int64  \n 37  h_6to17_single_m           23342 non-null  float64\n 38  emp_m                      23342 non-null  float64\n 39  memp_m                     23342 non-null  float64\n 40  femp_m                     23342 non-null  float64\n 41  emp_service                23342 non-null  float64\n 42  memp_service               23342 non-null  float64\n 43  femp_service               23342 non-null  float64\n 44  emp_sales                  23342 non-null  float64\n 45  memp_sales                 23342 non-null  float64\n 46  femp_sales                 23342 non-null  float64\n 47  emp_n                      23342 non-null  float64\n 48  memp_n                     23342 non-null  float64\n 49  femp_n                     23342 non-null  float64\n 50  emp_p                      23342 non-null  float64\n 51  memp_p                     23342 non-null  float64\n 52  femp_p                     23342 non-null  float64\n 53  mcsa                       23342 non-null  float64\n 54  mfccsa                     23342 non-null  float64\n 55  mc_infant                  23342 non-null  float64\n 56  mc_toddler                 23342 non-null  float64\n 57  mc_preschool               23342 non-null  float64\n 58  mfcc_infant                23342 non-null  float64\n 59  mfcc_toddler               23342 non-null  float64\n 60  mfcc_preschool             23342 non-null  float64\n 61  county_name                23342 non-null  object \n 62  state_name                 23342 non-null  object \n 63  state_abbreviation         23342 non-null  object \ndtypes: float64(51), int64(10), object(3)\nmemory usage: 11.4+ MB\n```\n:::\n:::\n\n\n#### Shape\n\nThe dataset contains **23341** ROWS and **64** COLUMNS.\n\n#### Columns\n\n1.  Categorical Columns : `country_name`, `state_name`, `state_abbreviation`.\n2.  Numerical Columns : `county_fips_code`, `study_year`, `one_race`, etc.\n( I am not going to type out all those columns. )\n\n#### DataTypes\n\n1.  `int64` : 10 column.\n2.  `float64` : 51 columns.\n3.  `object` : 03 columns.\n\n### Descriptive Statistics\n\n::: {#cell-eda-desc-stats .cell execution_count=5}\n``` {.python .cell-code}\ncosts_df.describe()\n```\n\n::: {#eda-desc-stats .cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>county_fips_code</th>\n      <th>study_year</th>\n      <th>unr_16</th>\n      <th>funr_16</th>\n      <th>munr_16</th>\n      <th>unr_20to64</th>\n      <th>funr_20to64</th>\n      <th>munr_20to64</th>\n      <th>flfpr_20to64</th>\n      <th>flfpr_20to64_under6</th>\n      <th>...</th>\n      <th>memp_p</th>\n      <th>femp_p</th>\n      <th>mcsa</th>\n      <th>mfccsa</th>\n      <th>mc_infant</th>\n      <th>mc_toddler</th>\n      <th>mc_preschool</th>\n      <th>mfcc_infant</th>\n      <th>mfcc_toddler</th>\n      <th>mfcc_preschool</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>...</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n      <td>23342.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>33273.162411</td>\n      <td>2013.422286</td>\n      <td>7.521682</td>\n      <td>7.061460</td>\n      <td>7.939197</td>\n      <td>6.962311</td>\n      <td>6.510894</td>\n      <td>7.369356</td>\n      <td>69.557325</td>\n      <td>68.569269</td>\n      <td>...</td>\n      <td>23.522334</td>\n      <td>7.360524</td>\n      <td>101.194718</td>\n      <td>92.469411</td>\n      <td>145.695372</td>\n      <td>130.094115</td>\n      <td>121.927485</td>\n      <td>113.388567</td>\n      <td>106.723178</td>\n      <td>104.153133</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>15581.597214</td>\n      <td>3.080393</td>\n      <td>3.550907</td>\n      <td>3.590273</td>\n      <td>4.036159</td>\n      <td>3.462389</td>\n      <td>3.504860</td>\n      <td>3.995685</td>\n      <td>7.880718</td>\n      <td>11.904121</td>\n      <td>...</td>\n      <td>7.657839</td>\n      <td>4.249184</td>\n      <td>34.573263</td>\n      <td>27.643789</td>\n      <td>53.554511</td>\n      <td>43.477543</td>\n      <td>38.336487</td>\n      <td>32.825649</td>\n      <td>29.982399</td>\n      <td>28.957938</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1001.000000</td>\n      <td>2008.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>35.100000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>20.190000</td>\n      <td>22.000000</td>\n      <td>36.300000</td>\n      <td>33.000000</td>\n      <td>33.000000</td>\n      <td>43.080000</td>\n      <td>43.080000</td>\n      <td>40.030000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>21043.000000</td>\n      <td>2011.000000</td>\n      <td>5.180000</td>\n      <td>4.690000</td>\n      <td>5.272500</td>\n      <td>4.700000</td>\n      <td>4.200000</td>\n      <td>4.772500</td>\n      <td>64.500000</td>\n      <td>62.200000</td>\n      <td>...</td>\n      <td>17.880000</td>\n      <td>4.340000</td>\n      <td>78.490000</td>\n      <td>75.000000</td>\n      <td>108.570000</td>\n      <td>100.000000</td>\n      <td>95.830000</td>\n      <td>90.000000</td>\n      <td>85.000000</td>\n      <td>84.212500</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>38005.000000</td>\n      <td>2014.000000</td>\n      <td>7.130000</td>\n      <td>6.630000</td>\n      <td>7.480000</td>\n      <td>6.570000</td>\n      <td>6.100000</td>\n      <td>6.900000</td>\n      <td>70.000000</td>\n      <td>69.400000</td>\n      <td>...</td>\n      <td>23.450000</td>\n      <td>6.390000</td>\n      <td>96.500000</td>\n      <td>88.050000</td>\n      <td>133.835000</td>\n      <td>120.665000</td>\n      <td>113.980000</td>\n      <td>106.000000</td>\n      <td>100.090000</td>\n      <td>99.650000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>48088.500000</td>\n      <td>2016.000000</td>\n      <td>9.420000</td>\n      <td>8.900000</td>\n      <td>10.040000</td>\n      <td>8.800000</td>\n      <td>8.300000</td>\n      <td>9.400000</td>\n      <td>75.100000</td>\n      <td>75.900000</td>\n      <td>...</td>\n      <td>28.820000</td>\n      <td>9.510000</td>\n      <td>119.337500</td>\n      <td>107.480000</td>\n      <td>165.010000</td>\n      <td>147.630000</td>\n      <td>138.095000</td>\n      <td>129.250000</td>\n      <td>124.620000</td>\n      <td>120.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>56045.000000</td>\n      <td>2018.000000</td>\n      <td>30.930000</td>\n      <td>38.240000</td>\n      <td>39.740000</td>\n      <td>33.600000</td>\n      <td>44.500000</td>\n      <td>45.500000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>...</td>\n      <td>87.500000</td>\n      <td>39.260000</td>\n      <td>375.400000</td>\n      <td>308.000000</td>\n      <td>470.000000</td>\n      <td>419.000000</td>\n      <td>385.000000</td>\n      <td>430.940000</td>\n      <td>376.320000</td>\n      <td>331.340000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 61 columns</p>\n</div>\n```\n:::\n:::\n\n\n### Column Separations for Future Use\n\n::: {#eda-column-separate .cell execution_count=6}\n``` {.python .cell-code}\n# Numerical Columns\nnumeric_cols = costs_df.select_dtypes(include = ['float64', 'int64']).columns\n\n# Categorical Columns\ncategoric_cols = costs_df.select_dtypes(include = ['object']).columns\n```\n:::\n\n\n## Task 2 - Data Preprocessing (1 Cr.)\n\n### Label Encoding\n\n::: {#cell-label-encoding .cell execution_count=7}\n``` {.python .cell-code}\nfor col in categoric_cols:\n    le = LabelEncoder()\n    costs_df[col] = le.fit_transform(costs_df[col])\n\ncosts_df.head()\n```\n\n::: {#label-encoding .cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>county_fips_code</th>\n      <th>study_year</th>\n      <th>unr_16</th>\n      <th>funr_16</th>\n      <th>munr_16</th>\n      <th>unr_20to64</th>\n      <th>funr_20to64</th>\n      <th>munr_20to64</th>\n      <th>flfpr_20to64</th>\n      <th>flfpr_20to64_under6</th>\n      <th>...</th>\n      <th>mfccsa</th>\n      <th>mc_infant</th>\n      <th>mc_toddler</th>\n      <th>mc_preschool</th>\n      <th>mfcc_infant</th>\n      <th>mfcc_toddler</th>\n      <th>mfcc_preschool</th>\n      <th>county_name</th>\n      <th>state_name</th>\n      <th>state_abbreviation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1001</td>\n      <td>2008</td>\n      <td>5.42</td>\n      <td>4.41</td>\n      <td>6.32</td>\n      <td>4.6</td>\n      <td>3.5</td>\n      <td>5.6</td>\n      <td>68.9</td>\n      <td>66.9</td>\n      <td>...</td>\n      <td>81.40</td>\n      <td>104.95</td>\n      <td>104.95</td>\n      <td>85.92</td>\n      <td>83.45</td>\n      <td>83.45</td>\n      <td>81.40</td>\n      <td>78</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001</td>\n      <td>2009</td>\n      <td>5.93</td>\n      <td>5.72</td>\n      <td>6.11</td>\n      <td>4.8</td>\n      <td>4.6</td>\n      <td>5.0</td>\n      <td>70.8</td>\n      <td>63.7</td>\n      <td>...</td>\n      <td>85.68</td>\n      <td>105.11</td>\n      <td>105.11</td>\n      <td>87.59</td>\n      <td>87.39</td>\n      <td>87.39</td>\n      <td>85.68</td>\n      <td>78</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1001</td>\n      <td>2010</td>\n      <td>6.21</td>\n      <td>5.57</td>\n      <td>6.78</td>\n      <td>5.1</td>\n      <td>4.6</td>\n      <td>5.6</td>\n      <td>71.3</td>\n      <td>67.0</td>\n      <td>...</td>\n      <td>89.96</td>\n      <td>105.28</td>\n      <td>105.28</td>\n      <td>89.26</td>\n      <td>91.33</td>\n      <td>91.33</td>\n      <td>89.96</td>\n      <td>78</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1001</td>\n      <td>2011</td>\n      <td>7.55</td>\n      <td>8.13</td>\n      <td>7.03</td>\n      <td>6.2</td>\n      <td>6.3</td>\n      <td>6.1</td>\n      <td>70.2</td>\n      <td>66.5</td>\n      <td>...</td>\n      <td>94.25</td>\n      <td>105.45</td>\n      <td>105.45</td>\n      <td>90.93</td>\n      <td>95.28</td>\n      <td>95.28</td>\n      <td>94.25</td>\n      <td>78</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1001</td>\n      <td>2012</td>\n      <td>8.60</td>\n      <td>8.88</td>\n      <td>8.29</td>\n      <td>6.7</td>\n      <td>6.4</td>\n      <td>7.0</td>\n      <td>70.6</td>\n      <td>67.1</td>\n      <td>...</td>\n      <td>98.53</td>\n      <td>105.61</td>\n      <td>105.61</td>\n      <td>92.60</td>\n      <td>99.22</td>\n      <td>99.22</td>\n      <td>98.53</td>\n      <td>78</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 64 columns</p>\n</div>\n```\n:::\n:::\n\n\n# 2 - Feature Engineering and Selection\n\n## Task 3 - Feature Engineering (1 Cr.)\n\n::: {#cell-feature-eng .cell execution_count=8}\n``` {.python .cell-code}\n# Grouping DF by 'state_name' & Calculating the Average 'mc_preschool' for Each State\nstate_avg = costs_df.groupby('state_name')['mc_preschool'].mean().reset_index()\nstate_avg.columns = ['state_name', 'avg_mc_preschool']\n\n# Merging the 'state_avg' with the OG DF\ncosts_df = costs_df.merge(state_avg, on = 'state_name')\n\n# Creating the 'above_state_avg' column (CONDITION : 1 if 'mc_preschool' is above 'state_avg', OTHERWISE 0\ncosts_df['above_state_avg'] = costs_df['mc_preschool'] > costs_df['avg_mc_preschool'].astype(int)\n\n# Dropping the now unneccessary 'avg_mc_preschool' column\ncosts_df = costs_df.drop(columns = ['avg_mc_preschool'])\n\ncosts_df.head()\n```\n\n::: {#feature-eng .cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>county_fips_code</th>\n      <th>study_year</th>\n      <th>unr_16</th>\n      <th>funr_16</th>\n      <th>munr_16</th>\n      <th>unr_20to64</th>\n      <th>funr_20to64</th>\n      <th>munr_20to64</th>\n      <th>flfpr_20to64</th>\n      <th>flfpr_20to64_under6</th>\n      <th>...</th>\n      <th>mc_infant</th>\n      <th>mc_toddler</th>\n      <th>mc_preschool</th>\n      <th>mfcc_infant</th>\n      <th>mfcc_toddler</th>\n      <th>mfcc_preschool</th>\n      <th>county_name</th>\n      <th>state_name</th>\n      <th>state_abbreviation</th>\n      <th>above_state_avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1001</td>\n      <td>2008</td>\n      <td>5.42</td>\n      <td>4.41</td>\n      <td>6.32</td>\n      <td>4.6</td>\n      <td>3.5</td>\n      <td>5.6</td>\n      <td>68.9</td>\n      <td>66.9</td>\n      <td>...</td>\n      <td>104.95</td>\n      <td>104.95</td>\n      <td>85.92</td>\n      <td>83.45</td>\n      <td>83.45</td>\n      <td>81.40</td>\n      <td>78</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001</td>\n      <td>2009</td>\n      <td>5.93</td>\n      <td>5.72</td>\n      <td>6.11</td>\n      <td>4.8</td>\n      <td>4.6</td>\n      <td>5.0</td>\n      <td>70.8</td>\n      <td>63.7</td>\n      <td>...</td>\n      <td>105.11</td>\n      <td>105.11</td>\n      <td>87.59</td>\n      <td>87.39</td>\n      <td>87.39</td>\n      <td>85.68</td>\n      <td>78</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1001</td>\n      <td>2010</td>\n      <td>6.21</td>\n      <td>5.57</td>\n      <td>6.78</td>\n      <td>5.1</td>\n      <td>4.6</td>\n      <td>5.6</td>\n      <td>71.3</td>\n      <td>67.0</td>\n      <td>...</td>\n      <td>105.28</td>\n      <td>105.28</td>\n      <td>89.26</td>\n      <td>91.33</td>\n      <td>91.33</td>\n      <td>89.96</td>\n      <td>78</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1001</td>\n      <td>2011</td>\n      <td>7.55</td>\n      <td>8.13</td>\n      <td>7.03</td>\n      <td>6.2</td>\n      <td>6.3</td>\n      <td>6.1</td>\n      <td>70.2</td>\n      <td>66.5</td>\n      <td>...</td>\n      <td>105.45</td>\n      <td>105.45</td>\n      <td>90.93</td>\n      <td>95.28</td>\n      <td>95.28</td>\n      <td>94.25</td>\n      <td>78</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1001</td>\n      <td>2012</td>\n      <td>8.60</td>\n      <td>8.88</td>\n      <td>8.29</td>\n      <td>6.7</td>\n      <td>6.4</td>\n      <td>7.0</td>\n      <td>70.6</td>\n      <td>67.1</td>\n      <td>...</td>\n      <td>105.61</td>\n      <td>105.61</td>\n      <td>92.60</td>\n      <td>99.22</td>\n      <td>99.22</td>\n      <td>98.53</td>\n      <td>78</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 65 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Task 4 - Feature Selection (1 Cr.)\n\n::: {#corr-matrix .cell execution_count=9}\n``` {.python .cell-code}\ncorr_matrix = costs_df[numeric_cols].corr()\n```\n:::\n\n\n### Heatmap of Numeric Features\n\n::: {#cell-numeric-heatmap .cell execution_count=10}\n``` {.python .cell-code}\nplt.figure(figsize = (8, 6))\nsns.heatmap(corr_matrix)\nplt.title(\"Correlation Heatmap of Numerical Features\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw-03-manasppanse_files/figure-html/numeric-heatmap-output-1.png){#numeric-heatmap width=745 height=630}\n:::\n:::\n\n\n### Highly Correlated Features\n\n::: {#cell-highly-correlated .cell execution_count=11}\n``` {.python .cell-code}\nthreshold = 0.8\nhigh_corr_pairs = corr_matrix.abs().unstack().sort_values(ascending = False).drop_duplicates()\nhigh_corr_pairs = high_corr_pairs[high_corr_pairs > threshold]\n\nhigh_corr_pairs\n```\n\n::: {#highly-correlated .cell-output .cell-output-display execution_count=11}\n```\ncounty_fips_code    county_fips_code     1.000000\ntwo_races           one_race             1.000000\ntotal_pop           households           0.996420\nh_under6_single_m   h_6to17_single_m     0.996162\nh_under6_both_work  h_6to17_both_work    0.995615\n                                           ...   \nmfccsa              mcsa                 0.827209\nme_2018             mhi_2018             0.825270\nmfccsa              mfcc_infant          0.824414\none_race_w          one_race_b           0.824256\nemp_service         memp_service         0.805360\nLength: 88, dtype: float64\n```\n:::\n:::\n\n\n# 3 - Model Implementation\n\n## Task 5 - Data Splitting (1 Cr.)\n\n::: {#dataset-split .cell execution_count=12}\n``` {.python .cell-code}\n# Defining Features and Target\nx = costs_df.drop(columns = ['above_state_avg'])\ny = costs_df['above_state_avg']\n\n# Dataset Split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n# Displaying Shapes of Datasets\nprint(f'x_train Shape : {x_train.shape}')\nprint(f'X_test Shape : {x_test.shape}')\nprint(f'y_train Shape : {y_train.shape}')\nprint(f'y_test Shape : {y_test.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx_train Shape : (18673, 64)\nX_test Shape : (4669, 64)\ny_train Shape : (18673,)\ny_test Shape : (4669,)\n```\n:::\n:::\n\n\n## Task 6 - Model Training (1 Cr.)\n\n### Logistic Regression\n\n::: {#logic-reg .cell execution_count=13}\n``` {.python .cell-code}\nlog_reg = LogisticRegression(max_iter = 1000)\nlog_reg.fit(x_train, y_train)\ny_pred_log_reg = log_reg.predict(x_test)\nlog_reg_acc = accuracy_score(y_test, y_pred_log_reg)\nprint(f'Logistic Regression Accuracy : {log_reg_acc:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Accuracy : 0.7190\n```\n:::\n:::\n\n\n### Decision Tree Classifier\n\n::: {#deci-tree-class .cell execution_count=14}\n``` {.python .cell-code}\ntree_clf = DecisionTreeClassifier(random_state = 42)\ntree_clf.fit(x_train, y_train)\ny_pred_tree = tree_clf.predict(x_test)\ntree_acc = accuracy_score(y_test, y_pred_tree)\nprint(f'Decision Tree Accuracy : {tree_acc:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDecision Tree Accuracy : 0.9715\n```\n:::\n:::\n\n\n### Random Forest Classifier\n\n::: {#ran-forest-class .cell execution_count=15}\n``` {.python .cell-code}\nforest_clf = RandomForestClassifier(random_state = 42)\nforest_clf.fit(x_train, y_train)\ny_pred_forest = forest_clf.predict(x_test)\nforest_acc = accuracy_score(y_test, y_pred_forest)\nprint(f'Random Forest Accuracy : {forest_acc:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Accuracy : 0.9822\n```\n:::\n:::\n\n\n### K - Nearest Neighbors Classifier\n\n::: {#k-neighbor-class .cell execution_count=16}\n``` {.python .cell-code}\nknn_clf = KNeighborsClassifier(n_neighbors = 5)\nknn_clf.fit(x_train, y_train)\ny_pred_knn = knn_clf.predict(x_test)\nknn_acc = accuracy_score(y_test, y_pred_knn)\nprint(f'K-Nearest Neighbors Accuracy : {knn_acc:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK-Nearest Neighbors Accuracy : 0.7933\n```\n:::\n:::\n\n\n# 4 - Model Evaluation and Interpretation\n\n## Task 7 - Model Validation (2 Cr.)\n\n::: {#score-cross-val .cell execution_count=17}\n``` {.python .cell-code}\nscoring = {\n    'accuracy': 'accuracy',\n    'precision': make_scorer(precision_score),\n    'recall': make_scorer(recall_score),\n    'f1': make_scorer(f1_score),\n    'roc_auc': make_scorer(roc_auc_score)\n}\n\n# Cross-Validation Function\ndef evaluate_model(model, x, y):\n    scores = cross_validate(model, x, y, cv = 5, scoring = scoring)\n    print(\"‾\" * 30)\n    print(f\"Model : {model.__class__.__name__}\")\n    print(f\"Accuracy : {scores['test_accuracy'].mean():.4f}\")\n    print(f\"Precision : {scores['test_precision'].mean():.4f}\")\n    print(f\"Recall : {scores['test_recall'].mean():.4f}\")\n    print(f\"F1-score : {scores['test_f1'].mean():.4f}\")\n    print(f\"ROC-AUC : {scores['test_roc_auc'].mean():.4f}\")\n    print(\"_\" * 30)\n```\n:::\n\n\n::: {#model-validate .cell execution_count=18}\n``` {.python .cell-code}\nlog_reg = LogisticRegression(max_iter = 1000)\nevaluate_model(log_reg, x_train, y_train)\n\ntree_clf = DecisionTreeClassifier(random_state = 42)\nevaluate_model(tree_clf, x_train, y_train)\n\nforest_clf = RandomForestClassifier(random_state = 42)\nevaluate_model(forest_clf, x_train, y_train)\n\nknn_clf = KNeighborsClassifier(n_neighbors = 5)\nevaluate_model(knn_clf, x_train, y_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : LogisticRegression\nAccuracy : 0.7068\nPrecision : 0.7170\nRecall : 0.4996\nF1-score : 0.5888\nROC-AUC : 0.6783\n______________________________\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : DecisionTreeClassifier\nAccuracy : 0.9504\nPrecision : 0.9429\nRecall : 0.9387\nF1-score : 0.9408\nROC-AUC : 0.9488\n______________________________\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : RandomForestClassifier\nAccuracy : 0.9765\nPrecision : 0.9678\nRecall : 0.9767\nF1-score : 0.9722\nROC-AUC : 0.9766\n______________________________\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : KNeighborsClassifier\nAccuracy : 0.7785\nPrecision : 0.7600\nRecall : 0.6909\nF1-score : 0.7238\nROC-AUC : 0.7664\n______________________________\n```\n:::\n:::\n\n\n## Task 8 - Result Interpretation (1 Cr.)\n\nIt's time to reveal the nominees for this assignment's Oscar for Best Performing Model: LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, AND last but not the least KNeighborsClassifier.\n\nAnd the Oscar goes to ... **RandomForestClassifier**. \n\n# 5 - Declaration of Independent Work\n\nSee **HOMEPAGE** for details\n\n",
    "supporting": [
      "hw-03-manasppanse_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}