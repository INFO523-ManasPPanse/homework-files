[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Common Rules Across HWs",
    "section": "",
    "text": "Instructions\n\nFor any exercise where you’re writing code, write your codes in the empty code chunk. You are also allowed to add additional code chunks when needed.\nFor any exercise where you’re creating a plot, make sure to label all axes, legends, etc. and give it an informative title.\nFor any exercise where you’re including a description and/or interpretation, use full sentences.\n\n\n\nHomework Submission Demo\nThere is a 7-min homework submission demo avaliable at Content &gt; Week 2. Please watch the video if you are unsure how to complete the homework.\n\n\nPolicies\n\nSharing / Reusing Code Policy : Unless explicitly stated otherwise, this course’s policy is that you may make use of any online resources (e.g. RStudio Community, StackOverflow, etc.) but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solution(s). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source.\nLate Submission Policy :\n\nLate, but next day (before noon): -10% of available points.\nLate, but next day (after noon): -20% of available points.\nTwo days late or later: No credit\n\nDeclaration of Independent Work : You must acknowledge that your submitted Assessment is your independent work, by putting your name and date at the end of the “Declaration of Independent Work” section.\n\n\n\nTotal Credits\n\n\n\nSr. No\nHomework Name\nAvailable Credits\n\n\n\n\n01\nHW-01\n10\n\n\n02\nHW-02\n10\n\n\n03\nHW-03\n10\n\n\n\n\n\nSubmission DDL\n\n\n\nSr. No\nHomework Name\nDDL\n\n\n\n\n01\nHW-01\nSep-09 12:01 AM The first minute of Week 3.\n\n\n02\nHW-02\nSep-23 12:01 AM The first minute of Week 5.\n\n\n03\nHW-03\nOct-13 23:59 PM The last minute of Week 7.\n\n\n\n\n\nDeclaration of Independent Work\nI hereby declare that this assignment is entirely my own work and that I have neither given nor received unauthorized assistance in completing it. I have adhered to all the guidelines provided for this assignment and have cited all sources from which I derived data, ideas, or words, whether quoted directly or paraphrased.\nFurthermore, I understand that providing false declaration is a violation of the University of Arizona’s honor code and will result in appropriate disciplinary action consistent with the severity of the violation.\n\n\n\nSr. No\nHomework Name\nDate\nSignature\n\n\n\n\n01\nHW-01\nSept 08th, 2024\nManas P Panse\n\n\n02\nHW-02\nSept 22th, 2024\nManas P Panse\n\n\n03\nHW-03\nOcto 13th, 2024\nManas P Panse"
  },
  {
    "objectID": "hw-02-manasppanse.html",
    "href": "hw-02-manasppanse.html",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "",
    "text": "# Checking Python Version\n\n!python --version\n\nPython 3.12.3\n\n\n\n# Importing Necessary Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport scipy.stats as stats\n\n\n# Importing Dataset\n\nsongs_df = pd.read_csv(\"data/hw-02/taylor_album_songs.csv\")"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-1---data-overview-1-cr.",
    "href": "hw-02-manasppanse.html#task-1---data-overview-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 1 - Data Overview (1 Cr.)",
    "text": "Task 1 - Data Overview (1 Cr.)\n\nsongs_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 194 entries, 0 to 193\nData columns (total 29 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   album_name           194 non-null    object \n 1   ep                   194 non-null    bool   \n 2   album_release        194 non-null    object \n 3   track_number         194 non-null    int64  \n 4   track_name           194 non-null    object \n 5   artist               191 non-null    object \n 6   featuring            16 non-null     object \n 7   bonus_track          194 non-null    bool   \n 8   promotional_release  18 non-null     object \n 9   single_release       36 non-null     object \n 10  track_release        194 non-null    object \n 11  danceability         191 non-null    float64\n 12  energy               191 non-null    float64\n 13  key                  191 non-null    float64\n 14  loudness             191 non-null    float64\n 15  mode                 191 non-null    float64\n 16  speechiness          191 non-null    float64\n 17  acousticness         191 non-null    float64\n 18  instrumentalness     191 non-null    float64\n 19  liveness             191 non-null    float64\n 20  valence              191 non-null    float64\n 21  tempo                191 non-null    float64\n 22  time_signature       191 non-null    float64\n 23  duration_ms          191 non-null    float64\n 24  explicit             191 non-null    object \n 25  key_name             191 non-null    object \n 26  mode_name            191 non-null    object \n 27  key_mode             191 non-null    object \n 28  lyrics               0 non-null      float64\ndtypes: bool(2), float64(14), int64(1), object(12)\nmemory usage: 41.4+ KB\n\n\n\nShape\nThe dataset contains 194 ROWS and 29 COLUMNS.\n\n\nColumns\n\nCategorical Columns : album_name, ep, album_release, track_name, artist, featuring, bonus_track, promotional_release, single_release, track_release, explicit, key_name, mode_name, key_mode.\nNumerical Columns : track_number, danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, time_signature, duration_ms, lyrics.\n\n\n\nDataTypes\n\nobject : 12 columns.\nbool : 02 columns.\nint64 : 01 column.\nfloat64 : 14 columns.\n\n\n\nDescriptive Statistics\n\nsongs_df.describe()\n\n\n\n\n\n\n\n\n\ntrack_number\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nduration_ms\nlyrics\n\n\n\n\ncount\n194.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n0.0\n\n\nmean\n10.706186\n0.584016\n0.574492\n4.685864\n-7.518058\n0.910995\n0.058310\n0.321225\n0.003936\n0.140813\n0.400857\n125.991424\n3.979058\n237078.942408\nNaN\n\n\nstd\n6.599675\n0.115372\n0.183443\n3.342826\n2.775821\n0.285500\n0.056866\n0.329021\n0.030607\n0.079951\n0.190456\n31.959403\n0.289430\n47316.125333\nNaN\n\n\nmin\n1.000000\n0.292000\n0.131000\n0.000000\n-15.434000\n0.000000\n0.023100\n0.000191\n0.000000\n0.035700\n0.038200\n68.534000\n1.000000\n148781.000000\nNaN\n\n\n25%\n5.000000\n0.511000\n0.446500\n2.000000\n-9.326000\n1.000000\n0.030800\n0.034600\n0.000000\n0.092950\n0.253500\n99.978000\n4.000000\n209326.500000\nNaN\n\n\n50%\n10.000000\n0.594000\n0.580000\n5.000000\n-6.937000\n1.000000\n0.039600\n0.162000\n0.000001\n0.115000\n0.404000\n121.956000\n4.000000\n232107.000000\nNaN\n\n\n75%\n15.000000\n0.652000\n0.717000\n7.000000\n-5.606000\n1.000000\n0.057400\n0.662000\n0.000040\n0.150500\n0.534500\n150.027500\n4.000000\n254447.500000\nNaN\n\n\nmax\n30.000000\n0.897000\n0.950000\n11.000000\n-2.098000\n1.000000\n0.519000\n0.971000\n0.348000\n0.594000\n0.942000\n208.918000\n5.000000\n613027.000000\nNaN"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-2---univariate-analysis-1-cr.",
    "href": "hw-02-manasppanse.html#task-2---univariate-analysis-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 2 - Univariate Analysis (1 Cr.)",
    "text": "Task 2 - Univariate Analysis (1 Cr.)\n\n# Numerical Columns\nnumeric_cols = songs_df.select_dtypes(include=['float64', 'int64']).columns\n\n# Categorical Columns\ncategoric_cols = songs_df.select_dtypes(include=['object', 'bool']).columns\n\n\nNumeric Columns Plot\n\nplt.figure(figsize = (15, 12))\nfor i, column in enumerate(numeric_cols, 1):\n  plt.subplot(4, 4, i)\n  sns.histplot(songs_df[column], kde = True)\n  plt.title(f'Histogram of {column}')\n  plt.xlabel(column)\n  plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCategoric Columns Plot\n\nplt.figure(figsize = (15, 12))\nfor i, column in enumerate(categoric_cols, 1):\n  plt.subplot(4, 4, i)\n  sns.histplot(songs_df[column], kde = True)\n  plt.title(f'Histogram of {column}')\n  plt.xlabel(column)\n  plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-3---bivariate-analysis-1-cr.",
    "href": "hw-02-manasppanse.html#task-3---bivariate-analysis-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 3 - Bivariate Analysis (1 Cr.)",
    "text": "Task 3 - Bivariate Analysis (1 Cr.)\n\nDanceability vs Energy\n\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = \"danceability\", y = \"energy\", data = songs_df, hue = \"album_name\")\nplt.title(\"Danceability vs Energy\", pad = 50)\nplt.legend(title = \"Album\", loc = 'upper center', bbox_to_anchor = (0.5, 1.15), fontsize = \"small\", ncol = 5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLoudness vs Acousticness\n\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = \"loudness\", y = \"acousticness\", data = songs_df, hue = \"album_name\")\nplt.title(\"Loudness vs Acousticness\", pad = 50)\nplt.legend(title = \"Album\", loc = 'upper center', bbox_to_anchor = (0.5, 1.15), fontsize = \"small\", ncol = 5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nValence vs Tempo\n\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = \"valence\", y = \"tempo\", data = songs_df, hue = \"album_name\")\nplt.title(\"Valence vs Tempo\", pad = 50)\nplt.legend(title = \"Album\", loc = 'upper center', bbox_to_anchor = (0.5, 1.15), fontsize = \"small\", ncol = 5)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-4---missing-data-outliers-1-cr.",
    "href": "hw-02-manasppanse.html#task-4---missing-data-outliers-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 4 - Missing Data & Outliers (1 Cr.)",
    "text": "Task 4 - Missing Data & Outliers (1 Cr.)\n\nNull Values\n\nsongs_df.isnull().sum()\n\nalbum_name               0\nep                       0\nalbum_release            0\ntrack_number             0\ntrack_name               0\nartist                   3\nfeaturing              178\nbonus_track              0\npromotional_release    176\nsingle_release         158\ntrack_release            0\ndanceability             3\nenergy                   3\nkey                      3\nloudness                 3\nmode                     3\nspeechiness              3\nacousticness             3\ninstrumentalness         3\nliveness                 3\nvalence                  3\ntempo                    3\ntime_signature           3\nduration_ms              3\nexplicit                 3\nkey_name                 3\nmode_name                3\nkey_mode                 3\nlyrics                 194\ndtype: int64\n\n\nColumns with NULLs : artist, featuring, promotional_release, single_release, danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, time_signature, duration_ms, lyrics.\n\n\nOutlier Detection\n\ndef find_outliers(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    return df[(df[column] &lt; lower_bound) | (df[column] &gt; upper_bound)]\n\noutliers_count = {col: len(find_outliers(songs_df, col)) for col in numeric_cols}\nprint(\"Total Outliers for each Numerical Column :\\n\", outliers_count)\n\nTotal Outliers for each Numerical Column :\n {'track_number': 0, 'danceability': 3, 'energy': 0, 'key': 0, 'loudness': 2, 'mode': 17, 'speechiness': 20, 'acousticness': 0, 'instrumentalness': 40, 'liveness': 21, 'valence': 0, 'tempo': 0, 'time_signature': 8, 'duration_ms': 5, 'lyrics': 0}"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-5---handling-missing-values-1-cr.",
    "href": "hw-02-manasppanse.html#task-5---handling-missing-values-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 5 - Handling Missing Values (1 Cr.)",
    "text": "Task 5 - Handling Missing Values (1 Cr.)\n\nfor col in numeric_cols:\n    songs_df[col].fillna(songs_df[col].median(), inplace = True)\n\n\nfor col in categoric_cols:\n    songs_df[col].fillna(songs_df[col].mode().iloc[0], inplace = True)\n\n\nsongs_df.isnull().sum()\n\nalbum_name               0\nep                       0\nalbum_release            0\ntrack_number             0\ntrack_name               0\nartist                   0\nfeaturing                0\nbonus_track              0\npromotional_release      0\nsingle_release           0\ntrack_release            0\ndanceability             0\nenergy                   0\nkey                      0\nloudness                 0\nmode                     0\nspeechiness              0\nacousticness             0\ninstrumentalness         0\nliveness                 0\nvalence                  0\ntempo                    0\ntime_signature           0\nduration_ms              0\nexplicit                 0\nkey_name                 0\nmode_name                0\nkey_mode                 0\nlyrics                 194\ndtype: int64"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-6---dealing-with-outliers-1-cr.",
    "href": "hw-02-manasppanse.html#task-6---dealing-with-outliers-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 6 - Dealing with Outliers (1 Cr.)",
    "text": "Task 6 - Dealing with Outliers (1 Cr.)\n\nplt.figure(figsize = (8, 6))\nsns.kdeplot(songs_df['danceability'], shade = True)\nplt.title('Density Plot of Danceability Before Capping')\nplt.show()\n\n\n\n\n\n\n\n\n\ntask6_lb = songs_df['danceability'].quantile(0.10) # Cutoff at 10th\ntask6_ub = songs_df['danceability'].quantile(0.90) # Cutoff at 90th\n\nsongs_df['danceability'] = songs_df['danceability'].clip(lower = task6_lb, upper = task6_ub)\n\n# Visualizing the Effect of Capping\nplt.figure(figsize = (8, 6))\nsns.kdeplot(songs_df['danceability'], shade = True)\nplt.title('Density Plot of Danceability After Capping')\nplt.show()"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-7---feature-engineering-1-cr.",
    "href": "hw-02-manasppanse.html#task-7---feature-engineering-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 7 - Feature Engineering (1 Cr.)",
    "text": "Task 7 - Feature Engineering (1 Cr.)\n\nsongs_df['valence_tempo_ratio'] = songs_df['valence'] / songs_df['tempo']\n\nprint(songs_df[['track_name', 'valence', 'tempo', 'valence_tempo_ratio']].head())\n\n               track_name  valence    tempo  valence_tempo_ratio\n0              Tim McGraw    0.425   76.009             0.005591\n1         Picture To Burn    0.821  105.586             0.007776\n2  Teardrops On My Guitar    0.289   99.953             0.002891\n3   A Place In This World    0.428  115.028             0.003721\n4             Cold As You    0.261  175.558             0.001487"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-8---data-transformation-1-cr.",
    "href": "hw-02-manasppanse.html#task-8---data-transformation-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 8 - Data Transformation (1 Cr.)",
    "text": "Task 8 - Data Transformation (1 Cr.)\n\nZ - Score Normalization\n\n# Z-Score Normalization\nscaler = StandardScaler()\nsongs_df['tempo_zscore'] = scaler.fit_transform(songs_df[['tempo']])\n\n# Transformation Check\nsongs_df['tempo_zscore'].describe()\n\ncount    1.940000e+02\nmean    -5.951711e-16\nstd      1.002587e+00\nmin     -1.814453e+00\n25%     -8.194760e-01\n50%     -1.256008e-01\n75%      7.578285e-01\nmax      2.623566e+00\nName: tempo_zscore, dtype: float64\n\n\n\n\nTransformation for Skewness\n\n# Density Plot\nplt.figure(figsize = (8, 6))\nsns.kdeplot(songs_df['loudness'], shade = True)\nplt.title('Density Plot of Loudness Before Transformation')\nplt.show()\n\n# Q-Q Plot\nplt.figure(figsize = (8, 6))\nstats.probplot(songs_df['loudness'], dist = \"norm\", plot = plt)\nplt.title('Q-Q Plot of Loudness Before Transformation')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Applying Log Transformation to Correct Skewness\nsongs_df['loudness_log'] = np.log1p(songs_df['loudness'] - songs_df['loudness'].min() + 1)\n\n\n# Density Plot\nplt.figure(figsize = (8, 6))\nsns.kdeplot(songs_df['loudness_log'], shade = True)\nplt.title('Density Plot of Loudness (After Log Transformation)')\nplt.show()\n\n# Q-Q Plot\nplt.figure(figsize = (8, 6))\nstats.probplot(songs_df['loudness_log'], dist = \"norm\", plot = plt)\nplt.title('Q-Q Plot of Loudness (After Log Transformation)')\nplt.show()"
  },
  {
    "objectID": "hw-01-manasppanse.html",
    "href": "hw-01-manasppanse.html",
    "title": "Python & NumPy Basics",
    "section": "",
    "text": "# Checking Python Version\n\n!python --version\n\nPython 3.12.3"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-1---variables-type-1-cr.",
    "href": "hw-01-manasppanse.html#task-1---variables-type-1-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 1 - Variables & Type (1 Cr.)",
    "text": "Task 1 - Variables & Type (1 Cr.)\n\nDefine two variables: an integer named age with a value of 25 and a string named course with the value “Data Mining”.\nPrint their values and types using the print() and the type() function.\n\n\n# Defining Variables\nage = 25\ncourse = \"Data Mining\"\n\n# Printing Values & Types\nprint(\"Value of 'age' Variable :\", age)\nprint(\"Type of 'age' Variable :\", type(age))\n\nprint(\"\\nValue of 'course' Variable :\", course)\nprint(\"Type of 'course' Variable :\", type(course))\n\nValue of 'age' Variable : 25\nType of 'age' Variable : &lt;class 'int'&gt;\n\nValue of 'course' Variable : Data Mining\nType of 'course' Variable : &lt;class 'str'&gt;"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-2---control-structures-2-cr.",
    "href": "hw-01-manasppanse.html#task-2---control-structures-2-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 2 - Control Structures (2 Cr.)",
    "text": "Task 2 - Control Structures (2 Cr.)\n\nWrite a function is_prime(num) that takes an integer and returns True if the number is a prime number, False otherwise.\nMake sure you include a loop and an appropriate control flow statement to check for primality.\n\n\n# Defining the Function\ndef is_prime(num):\n    if num &lt;= 1:\n        return False\n    for i in range(2, int(num ** 0.5) + 1):\n        if num % i == 0:\n            return False\n    return True\n\n# Testing\nprint(is_prime(1))\nprint(is_prime(2))\n\nFalse\nTrue"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-3---data-structures-2-cr.",
    "href": "hw-01-manasppanse.html#task-3---data-structures-2-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 3 - Data Structures (2 Cr.)",
    "text": "Task 3 - Data Structures (2 Cr.)\n\nCreate a dictionary named student_grades with keys as student names and values as their grades (A, B, C, D, F).\nWrite a loop to print out each student’s name and grade in the format: “Student [Name] has grade [Grade]”.\n\n\n# Creating a Dictionary named student_grades\nstudent_grades = {\n    'Manas': 'A',\n    'Bob': 'B',\n    'Jack': 'C',\n    'Mark': 'D',\n    'Wade': 'F'\n}\n\n# Looping a Print statement in the given format\nfor name, grade in student_grades.items():\n    print(f\"Student {name} has grade {grade}\")\n\nStudent Manas has grade A\nStudent Bob has grade B\nStudent Jack has grade C\nStudent Mark has grade D\nStudent Wade has grade F"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-4---numpy-arrays-1-cr.",
    "href": "hw-01-manasppanse.html#task-4---numpy-arrays-1-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 4 - NumPy Arrays (1 Cr.)",
    "text": "Task 4 - NumPy Arrays (1 Cr.)\n\nCreate a NumPy array A of shape (10,10) with values ranging from 0 to 99.\nCalculate the determinant of matrix A (use numpy.linalg.det). Print the result.\n\n\n# Creating the NumPy array `A`\nA = [\n  [47, 25,  8, 14, 61, 30, 87, 56,  9, 92],\n  [ 7, 66, 95, 42, 34, 77, 18, 54,  0, 81],\n  [38, 73, 64, 25,  5, 90, 16, 43, 57, 28],\n  [21, 50,  3, 87, 64, 29, 79, 94, 67, 41],\n  [ 4, 22, 18, 31, 96, 45, 72, 35, 60,  7],\n  [89, 62,  1, 74, 13, 68, 88, 27, 50, 12],\n  [76, 82,  5, 49, 53, 85, 33,  4, 24, 97],\n  [31,  6, 92, 78,  9, 46, 70, 19,  2, 83],\n  [65, 93, 28, 71, 40, 56,  7, 95, 82, 19],\n  [44, 91, 13, 52, 59, 37, 48,  6, 20, 85]\n]\n\n# Calculating the Determinant\ndeterminant = np.linalg.det(A)\n\n# Printing Result\nprint(f\"Determinant of the matrix A ( Δ ) : {determinant}\")\n\nDeterminant of the matrix A ( Δ ) : 1.0088849280721224e+19"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-5---iterate-over-elements-2-cr.",
    "href": "hw-01-manasppanse.html#task-5---iterate-over-elements-2-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 5 - Iterate over Elements (2 Cr.)",
    "text": "Task 5 - Iterate over Elements (2 Cr.)\n\nTract and print all the elements from the third column of a given 2D NumPy array.\nUse a for loop to iterate through each element of this column and print their square roots.\n\nGiven Code -\nimport numpy as np\n\n# Assuming a 2D array example\narray_2D = np.random.randint(1, 100, size=(5, 5))\n\n# complete the rest of codes here...\n\n# Creating a 2D Array\narray_2D = np.random.randint(1, 100, size=(5, 5))\n\n# Printing the Original Array\nprint(\"Original 2D Array : \")\nprint(array_2D)\n\n# Extracting the 3rd Column\nthird_column = array_2D[:, 2]\n\n# Printing the 3rd Column\nprint(\"\\nThird Column of the Array :\")\nprint(third_column)\n\n# Iterating through each Element of the 3rd Column and Printing their Square Roots\nprint(\"\\nSquare Roots of the Third Column Elements :\")\nfor element in third_column:\n    print(np.sqrt(element))\n\nOriginal 2D Array : \n[[81 55 16 49 94]\n [19 60 46 77 40]\n [63 49 30 95 21]\n [81  9 57 34 74]\n [84 10 95 63 93]]\n\nThird Column of the Array :\n[16 46 30 57 95]\n\nSquare Roots of the Third Column Elements :\n4.0\n6.782329983125268\n5.477225575051661\n7.54983443527075\n9.746794344808963"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-6-statistics-with-numpy-2-cr.",
    "href": "hw-01-manasppanse.html#task-6-statistics-with-numpy-2-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 6: Statistics with NumPy (2 Cr.)",
    "text": "Task 6: Statistics with NumPy (2 Cr.)\n\nGiven a 2D NumPy array, calculate the mean, median, and variance along both rows and columns.\nIdentify the row with the maximum variance and print it out.\n\n\n# Saving Timg and Borrowing the original 2D array from TASK 5\nprint(\"Original 2D Array :\")\nprint(array_2D)\n\nOriginal 2D Array :\n[[81 55 16 49 94]\n [19 60 46 77 40]\n [63 49 30 95 21]\n [81  9 57 34 74]\n [84 10 95 63 93]]\n\n\n\n# Calculating Mean along Columns (axis = 0)\nmean_columns = np.mean(array_2D, axis=0)\nprint(\"Mean along Columns :\")\nprint(mean_columns)\n\n# Calculating Mean along Rows (axis = 1)\nmean_rows = np.mean(array_2D, axis=1)\nprint(\"\\nMean along Rows :\")\nprint(mean_rows)\n\nMean along Columns :\n[65.6 36.6 48.8 63.6 64.4]\n\nMean along Rows :\n[59.  48.4 51.6 51.  69. ]\n\n\n\n# Median along Columns (axis = 0)\nmedian_columns = np.median(array_2D, axis=0)\nprint(\"Median along Columns :\")\nprint(median_columns)\n\n# Median along Rows (axis = 1)\nmedian_rows = np.median(array_2D, axis=1)\nprint(\"\\nMedian along Rows :\")\nprint(median_rows)\n\nMedian along Columns :\n[81. 49. 46. 63. 74.]\n\nMedian along Rows :\n[55. 46. 49. 57. 84.]\n\n\n\n# Variance along columns (axis=0)\nvariance_columns = np.var(array_2D, axis=0)\nprint(\"Variance along Columns :\")\nprint(variance_columns)\n\n# Variance along rows (axis=1)\nvariance_rows = np.var(array_2D, axis=1)\nprint(\"\\nVariance along Rows :\")\nprint(variance_rows)\n\nVariance along Columns :\n[598.24 501.84 727.76 451.04 853.04]\n\nVariance along Rows :\n[734.8  378.64 684.64 703.6  998.8 ]\n\n\n\n# Identifying the Row with the MAXIMUM Variance\nmax_variance_row_index = np.argmax(variance_rows)\nmax_variance_row = array_2D[max_variance_row_index]\n\n# Print the Row with the MAXIMUM Variance\nprint(\"Row with the MAX Variance :\")\nprint(max_variance_row)\n\nRow with the MAX Variance :\n[84 10 95 63 93]"
  },
  {
    "objectID": "hw-03-manasppanse.html",
    "href": "hw-03-manasppanse.html",
    "title": "Classification & Model Evaluation",
    "section": "",
    "text": "# Checking Python Version\n\n!python --version\n\nPython 3.12.3\n\n\n\n# Importing Necessary Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score, make_scorer\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score\nfrom sklearn.model_selection import cross_validate, cross_val_score, StratifiedKFold, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\n\n# Importing Dataset\n\ncosts_df = pd.read_csv(\"data/hw-03/childcare_costs.csv\")"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-1---exploratory-data-analysis-1-cr.",
    "href": "hw-03-manasppanse.html#task-1---exploratory-data-analysis-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 1 - Exploratory Data Analysis (1 Cr.)",
    "text": "Task 1 - Exploratory Data Analysis (1 Cr.)\n\nData Overview\n\ncosts_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 23342 entries, 0 to 23341\nData columns (total 64 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   county_fips_code           23342 non-null  int64  \n 1   study_year                 23342 non-null  int64  \n 2   unr_16                     23342 non-null  float64\n 3   funr_16                    23342 non-null  float64\n 4   munr_16                    23342 non-null  float64\n 5   unr_20to64                 23342 non-null  float64\n 6   funr_20to64                23342 non-null  float64\n 7   munr_20to64                23342 non-null  float64\n 8   flfpr_20to64               23342 non-null  float64\n 9   flfpr_20to64_under6        23342 non-null  float64\n 10  flfpr_20to64_6to17         23342 non-null  float64\n 11  flfpr_20to64_under6_6to17  23342 non-null  float64\n 12  mlfpr_20to64               23342 non-null  float64\n 13  pr_f                       23342 non-null  float64\n 14  pr_p                       23342 non-null  float64\n 15  mhi_2018                   23342 non-null  float64\n 16  me_2018                    23342 non-null  float64\n 17  fme_2018                   23342 non-null  float64\n 18  mme_2018                   23342 non-null  float64\n 19  total_pop                  23342 non-null  int64  \n 20  one_race                   23342 non-null  float64\n 21  one_race_w                 23342 non-null  float64\n 22  one_race_b                 23342 non-null  float64\n 23  one_race_i                 23342 non-null  float64\n 24  one_race_a                 23342 non-null  float64\n 25  one_race_h                 23342 non-null  float64\n 26  one_race_other             23342 non-null  float64\n 27  two_races                  23342 non-null  float64\n 28  hispanic                   23342 non-null  float64\n 29  households                 23342 non-null  int64  \n 30  h_under6_both_work         23342 non-null  int64  \n 31  h_under6_f_work            23342 non-null  int64  \n 32  h_under6_m_work            23342 non-null  int64  \n 33  h_under6_single_m          23342 non-null  float64\n 34  h_6to17_both_work          23342 non-null  int64  \n 35  h_6to17_fwork              23342 non-null  int64  \n 36  h_6to17_mwork              23342 non-null  int64  \n 37  h_6to17_single_m           23342 non-null  float64\n 38  emp_m                      23342 non-null  float64\n 39  memp_m                     23342 non-null  float64\n 40  femp_m                     23342 non-null  float64\n 41  emp_service                23342 non-null  float64\n 42  memp_service               23342 non-null  float64\n 43  femp_service               23342 non-null  float64\n 44  emp_sales                  23342 non-null  float64\n 45  memp_sales                 23342 non-null  float64\n 46  femp_sales                 23342 non-null  float64\n 47  emp_n                      23342 non-null  float64\n 48  memp_n                     23342 non-null  float64\n 49  femp_n                     23342 non-null  float64\n 50  emp_p                      23342 non-null  float64\n 51  memp_p                     23342 non-null  float64\n 52  femp_p                     23342 non-null  float64\n 53  mcsa                       23342 non-null  float64\n 54  mfccsa                     23342 non-null  float64\n 55  mc_infant                  23342 non-null  float64\n 56  mc_toddler                 23342 non-null  float64\n 57  mc_preschool               23342 non-null  float64\n 58  mfcc_infant                23342 non-null  float64\n 59  mfcc_toddler               23342 non-null  float64\n 60  mfcc_preschool             23342 non-null  float64\n 61  county_name                23342 non-null  object \n 62  state_name                 23342 non-null  object \n 63  state_abbreviation         23342 non-null  object \ndtypes: float64(51), int64(10), object(3)\nmemory usage: 11.4+ MB\n\n\n\nShape\nThe dataset contains 23341 ROWS and 64 COLUMNS.\n\n\nColumns\n\nCategorical Columns : country_name, state_name, state_abbreviation.\nNumerical Columns : county_fips_code, study_year, one_race, etc. ( I am not going to type out all those columns. )\n\n\n\nDataTypes\n\nint64 : 10 column.\nfloat64 : 51 columns.\nobject : 03 columns.\n\n\n\n\nDescriptive Statistics\n\ncosts_df.describe()\n\n\n\n\n\n\n\n\n\ncounty_fips_code\nstudy_year\nunr_16\nfunr_16\nmunr_16\nunr_20to64\nfunr_20to64\nmunr_20to64\nflfpr_20to64\nflfpr_20to64_under6\n...\nmemp_p\nfemp_p\nmcsa\nmfccsa\nmc_infant\nmc_toddler\nmc_preschool\nmfcc_infant\nmfcc_toddler\nmfcc_preschool\n\n\n\n\ncount\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n...\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n\n\nmean\n33273.162411\n2013.422286\n7.521682\n7.061460\n7.939197\n6.962311\n6.510894\n7.369356\n69.557325\n68.569269\n...\n23.522334\n7.360524\n101.194718\n92.469411\n145.695372\n130.094115\n121.927485\n113.388567\n106.723178\n104.153133\n\n\nstd\n15581.597214\n3.080393\n3.550907\n3.590273\n4.036159\n3.462389\n3.504860\n3.995685\n7.880718\n11.904121\n...\n7.657839\n4.249184\n34.573263\n27.643789\n53.554511\n43.477543\n38.336487\n32.825649\n29.982399\n28.957938\n\n\nmin\n1001.000000\n2008.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n35.100000\n0.000000\n...\n0.000000\n0.000000\n20.190000\n22.000000\n36.300000\n33.000000\n33.000000\n43.080000\n43.080000\n40.030000\n\n\n25%\n21043.000000\n2011.000000\n5.180000\n4.690000\n5.272500\n4.700000\n4.200000\n4.772500\n64.500000\n62.200000\n...\n17.880000\n4.340000\n78.490000\n75.000000\n108.570000\n100.000000\n95.830000\n90.000000\n85.000000\n84.212500\n\n\n50%\n38005.000000\n2014.000000\n7.130000\n6.630000\n7.480000\n6.570000\n6.100000\n6.900000\n70.000000\n69.400000\n...\n23.450000\n6.390000\n96.500000\n88.050000\n133.835000\n120.665000\n113.980000\n106.000000\n100.090000\n99.650000\n\n\n75%\n48088.500000\n2016.000000\n9.420000\n8.900000\n10.040000\n8.800000\n8.300000\n9.400000\n75.100000\n75.900000\n...\n28.820000\n9.510000\n119.337500\n107.480000\n165.010000\n147.630000\n138.095000\n129.250000\n124.620000\n120.000000\n\n\nmax\n56045.000000\n2018.000000\n30.930000\n38.240000\n39.740000\n33.600000\n44.500000\n45.500000\n100.000000\n100.000000\n...\n87.500000\n39.260000\n375.400000\n308.000000\n470.000000\n419.000000\n385.000000\n430.940000\n376.320000\n331.340000\n\n\n\n\n8 rows × 61 columns\n\n\n\n\n\n\nColumn Separations for Future Use\n\n# Numerical Columns\nnumeric_cols = costs_df.select_dtypes(include = ['float64', 'int64']).columns\n\n# Categorical Columns\ncategoric_cols = costs_df.select_dtypes(include = ['object']).columns"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-2---data-preprocessing-1-cr.",
    "href": "hw-03-manasppanse.html#task-2---data-preprocessing-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 2 - Data Preprocessing (1 Cr.)",
    "text": "Task 2 - Data Preprocessing (1 Cr.)\n\nLabel Encoding\n\nfor col in categoric_cols:\n    le = LabelEncoder()\n    costs_df[col] = le.fit_transform(costs_df[col])\n\ncosts_df.head()\n\n\n\n\n\n\n\n\n\ncounty_fips_code\nstudy_year\nunr_16\nfunr_16\nmunr_16\nunr_20to64\nfunr_20to64\nmunr_20to64\nflfpr_20to64\nflfpr_20to64_under6\n...\nmfccsa\nmc_infant\nmc_toddler\nmc_preschool\nmfcc_infant\nmfcc_toddler\nmfcc_preschool\ncounty_name\nstate_name\nstate_abbreviation\n\n\n\n\n0\n1001\n2008\n5.42\n4.41\n6.32\n4.6\n3.5\n5.6\n68.9\n66.9\n...\n81.40\n104.95\n104.95\n85.92\n83.45\n83.45\n81.40\n78\n0\n1\n\n\n1\n1001\n2009\n5.93\n5.72\n6.11\n4.8\n4.6\n5.0\n70.8\n63.7\n...\n85.68\n105.11\n105.11\n87.59\n87.39\n87.39\n85.68\n78\n0\n1\n\n\n2\n1001\n2010\n6.21\n5.57\n6.78\n5.1\n4.6\n5.6\n71.3\n67.0\n...\n89.96\n105.28\n105.28\n89.26\n91.33\n91.33\n89.96\n78\n0\n1\n\n\n3\n1001\n2011\n7.55\n8.13\n7.03\n6.2\n6.3\n6.1\n70.2\n66.5\n...\n94.25\n105.45\n105.45\n90.93\n95.28\n95.28\n94.25\n78\n0\n1\n\n\n4\n1001\n2012\n8.60\n8.88\n8.29\n6.7\n6.4\n7.0\n70.6\n67.1\n...\n98.53\n105.61\n105.61\n92.60\n99.22\n99.22\n98.53\n78\n0\n1\n\n\n\n\n5 rows × 64 columns"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-3---feature-engineering-1-cr.",
    "href": "hw-03-manasppanse.html#task-3---feature-engineering-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 3 - Feature Engineering (1 Cr.)",
    "text": "Task 3 - Feature Engineering (1 Cr.)\n\n# Grouping DF by 'state_name' & Calculating the Average 'mc_preschool' for Each State\nstate_avg = costs_df.groupby('state_name')['mc_preschool'].mean().reset_index()\nstate_avg.columns = ['state_name', 'avg_mc_preschool']\n\n# Merging the 'state_avg' with the OG DF\ncosts_df = costs_df.merge(state_avg, on = 'state_name')\n\n# Creating the 'above_state_avg' column (CONDITION : 1 if 'mc_preschool' is above 'state_avg', OTHERWISE 0\ncosts_df['above_state_avg'] = costs_df['mc_preschool'] &gt; costs_df['avg_mc_preschool'].astype(int)\n\n# Dropping the now unneccessary 'avg_mc_preschool' column\ncosts_df = costs_df.drop(columns = ['avg_mc_preschool'])\n\ncosts_df.head()\n\n\n\n\n\n\n\n\n\ncounty_fips_code\nstudy_year\nunr_16\nfunr_16\nmunr_16\nunr_20to64\nfunr_20to64\nmunr_20to64\nflfpr_20to64\nflfpr_20to64_under6\n...\nmc_infant\nmc_toddler\nmc_preschool\nmfcc_infant\nmfcc_toddler\nmfcc_preschool\ncounty_name\nstate_name\nstate_abbreviation\nabove_state_avg\n\n\n\n\n0\n1001\n2008\n5.42\n4.41\n6.32\n4.6\n3.5\n5.6\n68.9\n66.9\n...\n104.95\n104.95\n85.92\n83.45\n83.45\n81.40\n78\n0\n1\nFalse\n\n\n1\n1001\n2009\n5.93\n5.72\n6.11\n4.8\n4.6\n5.0\n70.8\n63.7\n...\n105.11\n105.11\n87.59\n87.39\n87.39\n85.68\n78\n0\n1\nFalse\n\n\n2\n1001\n2010\n6.21\n5.57\n6.78\n5.1\n4.6\n5.6\n71.3\n67.0\n...\n105.28\n105.28\n89.26\n91.33\n91.33\n89.96\n78\n0\n1\nFalse\n\n\n3\n1001\n2011\n7.55\n8.13\n7.03\n6.2\n6.3\n6.1\n70.2\n66.5\n...\n105.45\n105.45\n90.93\n95.28\n95.28\n94.25\n78\n0\n1\nFalse\n\n\n4\n1001\n2012\n8.60\n8.88\n8.29\n6.7\n6.4\n7.0\n70.6\n67.1\n...\n105.61\n105.61\n92.60\n99.22\n99.22\n98.53\n78\n0\n1\nFalse\n\n\n\n\n5 rows × 65 columns"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-4---feature-selection-1-cr.",
    "href": "hw-03-manasppanse.html#task-4---feature-selection-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 4 - Feature Selection (1 Cr.)",
    "text": "Task 4 - Feature Selection (1 Cr.)\n\ncorr_matrix = costs_df[numeric_cols].corr()\n\n\nHeatmap of Numeric Features\n\nplt.figure(figsize = (8, 6))\nsns.heatmap(corr_matrix)\nplt.title(\"Correlation Heatmap of Numerical Features\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHighly Correlated Features\n\nthreshold = 0.8\nhigh_corr_pairs = corr_matrix.abs().unstack().sort_values(ascending = False).drop_duplicates()\nhigh_corr_pairs = high_corr_pairs[high_corr_pairs &gt; threshold]\n\nhigh_corr_pairs\n\ncounty_fips_code    county_fips_code     1.000000\ntwo_races           one_race             1.000000\ntotal_pop           households           0.996420\nh_under6_single_m   h_6to17_single_m     0.996162\nh_under6_both_work  h_6to17_both_work    0.995615\n                                           ...   \nmfccsa              mcsa                 0.827209\nme_2018             mhi_2018             0.825270\nmfccsa              mfcc_infant          0.824414\none_race_w          one_race_b           0.824256\nemp_service         memp_service         0.805360\nLength: 88, dtype: float64"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-5---data-splitting-1-cr.",
    "href": "hw-03-manasppanse.html#task-5---data-splitting-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 5 - Data Splitting (1 Cr.)",
    "text": "Task 5 - Data Splitting (1 Cr.)\n\n# Defining Features and Target\nx = costs_df.drop(columns = ['above_state_avg'])\ny = costs_df['above_state_avg']\n\n# Dataset Split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n# Displaying Shapes of Datasets\nprint(f'x_train Shape : {x_train.shape}')\nprint(f'X_test Shape : {x_test.shape}')\nprint(f'y_train Shape : {y_train.shape}')\nprint(f'y_test Shape : {y_test.shape}')\n\nx_train Shape : (18673, 64)\nX_test Shape : (4669, 64)\ny_train Shape : (18673,)\ny_test Shape : (4669,)"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-6---model-training-1-cr.",
    "href": "hw-03-manasppanse.html#task-6---model-training-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 6 - Model Training (1 Cr.)",
    "text": "Task 6 - Model Training (1 Cr.)\n\nLogistic Regression\n\nlog_reg = LogisticRegression(max_iter = 1000)\nlog_reg.fit(x_train, y_train)\ny_pred_log_reg = log_reg.predict(x_test)\nlog_reg_acc = accuracy_score(y_test, y_pred_log_reg)\nprint(f'Logistic Regression Accuracy : {log_reg_acc:.4f}')\n\nLogistic Regression Accuracy : 0.7190\n\n\n\n\nDecision Tree Classifier\n\ntree_clf = DecisionTreeClassifier(random_state = 42)\ntree_clf.fit(x_train, y_train)\ny_pred_tree = tree_clf.predict(x_test)\ntree_acc = accuracy_score(y_test, y_pred_tree)\nprint(f'Decision Tree Accuracy : {tree_acc:.4f}')\n\nDecision Tree Accuracy : 0.9715\n\n\n\n\nRandom Forest Classifier\n\nforest_clf = RandomForestClassifier(random_state = 42)\nforest_clf.fit(x_train, y_train)\ny_pred_forest = forest_clf.predict(x_test)\nforest_acc = accuracy_score(y_test, y_pred_forest)\nprint(f'Random Forest Accuracy : {forest_acc:.4f}')\n\nRandom Forest Accuracy : 0.9822\n\n\n\n\nK - Nearest Neighbors Classifier\n\nknn_clf = KNeighborsClassifier(n_neighbors = 5)\nknn_clf.fit(x_train, y_train)\ny_pred_knn = knn_clf.predict(x_test)\nknn_acc = accuracy_score(y_test, y_pred_knn)\nprint(f'K-Nearest Neighbors Accuracy : {knn_acc:.4f}')\n\nK-Nearest Neighbors Accuracy : 0.7933"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-7---model-validation-2-cr.",
    "href": "hw-03-manasppanse.html#task-7---model-validation-2-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 7 - Model Validation (2 Cr.)",
    "text": "Task 7 - Model Validation (2 Cr.)\n\nscoring = {\n    'accuracy': 'accuracy',\n    'precision': make_scorer(precision_score),\n    'recall': make_scorer(recall_score),\n    'f1': make_scorer(f1_score),\n    'roc_auc': make_scorer(roc_auc_score)\n}\n\n# Cross-Validation Function\ndef evaluate_model(model, x, y):\n    scores = cross_validate(model, x, y, cv = 5, scoring = scoring)\n    print(\"‾\" * 30)\n    print(f\"Model : {model.__class__.__name__}\")\n    print(f\"Accuracy : {scores['test_accuracy'].mean():.4f}\")\n    print(f\"Precision : {scores['test_precision'].mean():.4f}\")\n    print(f\"Recall : {scores['test_recall'].mean():.4f}\")\n    print(f\"F1-score : {scores['test_f1'].mean():.4f}\")\n    print(f\"ROC-AUC : {scores['test_roc_auc'].mean():.4f}\")\n    print(\"_\" * 30)\n\n\nlog_reg = LogisticRegression(max_iter = 1000)\nevaluate_model(log_reg, x_train, y_train)\n\ntree_clf = DecisionTreeClassifier(random_state = 42)\nevaluate_model(tree_clf, x_train, y_train)\n\nforest_clf = RandomForestClassifier(random_state = 42)\nevaluate_model(forest_clf, x_train, y_train)\n\nknn_clf = KNeighborsClassifier(n_neighbors = 5)\nevaluate_model(knn_clf, x_train, y_train)\n\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : LogisticRegression\nAccuracy : 0.7068\nPrecision : 0.7170\nRecall : 0.4996\nF1-score : 0.5888\nROC-AUC : 0.6783\n______________________________\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : DecisionTreeClassifier\nAccuracy : 0.9504\nPrecision : 0.9429\nRecall : 0.9387\nF1-score : 0.9408\nROC-AUC : 0.9488\n______________________________\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : RandomForestClassifier\nAccuracy : 0.9765\nPrecision : 0.9678\nRecall : 0.9767\nF1-score : 0.9722\nROC-AUC : 0.9766\n______________________________\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : KNeighborsClassifier\nAccuracy : 0.7785\nPrecision : 0.7600\nRecall : 0.6909\nF1-score : 0.7238\nROC-AUC : 0.7664\n______________________________"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-8---result-interpretation-1-cr.",
    "href": "hw-03-manasppanse.html#task-8---result-interpretation-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 8 - Result Interpretation (1 Cr.)",
    "text": "Task 8 - Result Interpretation (1 Cr.)\nIt’s time to reveal the nominees for this assignment’s Oscar for Best Performing Model: LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, AND last but not the least KNeighborsClassifier.\nAnd the Oscar goes to … RandomForestClassifier."
  }
]