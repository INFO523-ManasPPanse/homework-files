[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Common Rules Across HWs",
    "section": "",
    "text": "Instructions\n\nFor any exercise where you’re writing code, write your codes in the empty code chunk. You are also allowed to add additional code chunks when needed.\nFor any exercise where you’re creating a plot, make sure to label all axes, legends, etc. and give it an informative title.\nFor any exercise where you’re including a description and/or interpretation, use full sentences.\n\n\n\nHomework Submission Demo\nThere is a 7-min homework submission demo avaliable at Content &gt; Week 2. Please watch the video if you are unsure how to complete the homework.\n\n\nPolicies\n\nSharing / Reusing Code Policy : Unless explicitly stated otherwise, this course’s policy is that you may make use of any online resources (e.g. RStudio Community, StackOverflow, etc.) but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solution(s). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source.\nLate Submission Policy :\n\nLate, but next day (before noon): -10% of available points.\nLate, but next day (after noon): -20% of available points.\nTwo days late or later: No credit\n\nDeclaration of Independent Work : You must acknowledge that your submitted Assessment is your independent work, by putting your name and date at the end of the “Declaration of Independent Work” section.\n\n\n\nTotal Credits\n\n\n\nSr. No\nHomework Name\nAvailable Credits\n\n\n\n\n01\nHW-01\n10\n\n\n02\nHW-02\n10\n\n\n03\nHW-03\n10\n\n\n04\nHW-04\n10\n\n\n05\nHW-05\n10\n\n\n\n\n\nSubmission DDL\n\n\n\nSr. No\nHomework Name\nDDL\n\n\n\n\n01\nHW-01\nSep-09 12:01 AM The first minute of Week 3.\n\n\n02\nHW-02\nSep-23 12:01 AM The first minute of Week 5.\n\n\n03\nHW-03\nOct-13 23:59 PM The last minute of Week 7.\n\n\n04\nHW-04\nNov-03 23:59 PM The last minute of Week 10.\n\n\n05\nHW-05\nNov-24 23:59 PM The last minute of Week 14.\n\n\n\n\n\nDeclaration of Independent Work\nI hereby declare that this assignment is entirely my own work and that I have neither given nor received unauthorized assistance in completing it. I have adhered to all the guidelines provided for this assignment and have cited all sources from which I derived data, ideas, or words, whether quoted directly or paraphrased.\nFurthermore, I understand that providing false declaration is a violation of the University of Arizona’s honor code and will result in appropriate disciplinary action consistent with the severity of the violation.\n\n\n\nSr. No\nHomework Name\nDate\nSignature\n\n\n\n\n01\nHW-01\nSept 08th, 2024\nManas P Panse\n\n\n02\nHW-02\nSept 22th, 2024\nManas P Panse\n\n\n03\nHW-03\nOcto 13th, 2024\nManas P Panse\n\n\n04\nHW-04\nNove 01st, 2024\nManas P Panse\n\n\n05\nHW-05\nNove 22st, 2024\nManas P Panse"
  },
  {
    "objectID": "hw-04-manasppanse.html",
    "href": "hw-04-manasppanse.html",
    "title": "Regression Models",
    "section": "",
    "text": "# Checking Python Version\n\n!python --version\n\nPython 3.12.3\n\n\n\n# Importing Necessary Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nsns.set(style = \"white\")\n\n\n# Importing Dataset\n\nsurvival_df = pd.read_csv(\"data/hw-04/survivalists.csv\")"
  },
  {
    "objectID": "hw-04-manasppanse.html#task-1---exploratory-data-analysis-1-cr.",
    "href": "hw-04-manasppanse.html#task-1---exploratory-data-analysis-1-cr.",
    "title": "Regression Models",
    "section": "Task 1 - Exploratory Data Analysis (1 Cr.)",
    "text": "Task 1 - Exploratory Data Analysis (1 Cr.)\n\nData Overview\n\nsurvival_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 94 entries, 0 to 93\nData columns (total 16 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   season               94 non-null     int64  \n 1   name                 94 non-null     object \n 2   age                  94 non-null     int64  \n 3   gender               94 non-null     object \n 4   city                 94 non-null     object \n 5   state                93 non-null     object \n 6   country              94 non-null     object \n 7   result               94 non-null     int64  \n 8   days_lasted          94 non-null     int64  \n 9   medically_evacuated  94 non-null     bool   \n 10  reason_tapped_out    84 non-null     object \n 11  reason_category      84 non-null     object \n 12  team                 14 non-null     object \n 13  day_linked_up        8 non-null      float64\n 14  profession           94 non-null     object \n 15  url                  94 non-null     object \ndtypes: bool(1), float64(1), int64(4), object(10)\nmemory usage: 11.2+ KB\n\n\n\nShape\nThe dataset contains 94 ROWS and 16 COLUMNS.\n\n\nColumns\n\nCategorical Columns : name, gender, city, state, country, medically_evacuated, reason_tapped_out, reason_category, team, profession, & url.\nNumerical Columns : season, age, result, days_lasted, & day_linked_up.\n\n[Going to include the bool as a categoric value.]\n\n\nDataTypes\n\nint64 : 04 column.\nobject : 10 columns.\nbool : 01 column.\nfloat64 : 01 columns.\n\n\n\n\nDescriptive Statistics\n\nsurvival_df.describe()\n\n\n\n\n\n\n\n\n\nseason\nage\nresult\ndays_lasted\nday_linked_up\n\n\n\n\ncount\n94.000000\n94.00000\n94.000000\n94.000000\n8.000000\n\n\nmean\n4.957447\n37.93617\n5.276596\n39.042553\n9.000000\n\n\nstd\n2.548096\n8.84491\n2.826161\n27.849409\n0.755929\n\n\nmin\n1.000000\n19.00000\n1.000000\n0.000000\n8.000000\n\n\n25%\n3.000000\n31.00000\n3.000000\n10.500000\n8.750000\n\n\n50%\n5.000000\n38.50000\n5.000000\n39.500000\n9.000000\n\n\n75%\n7.000000\n44.00000\n7.750000\n63.750000\n9.250000\n\n\nmax\n9.000000\n61.00000\n10.000000\n100.000000\n10.000000\n\n\n\n\n\n\n\n\n\n\nColumn Separations for Future Use\n\n# Numerical Columns\nnumeric_cols = survival_df.select_dtypes(include = ['int64', 'float64']).columns\n\n# Categorical Columns\ncategoric_cols = survival_df.select_dtypes(include = ['object', 'bool']).columns\n\n\n\nDistribution of days_lasted\n\nplt.figure(figsize = (8, 6))\ndl = sns.histplot(survival_df['days_lasted'], kde = True, bins = 20, color = 'lime')\ndl.lines[0].set_color('red')\nplt.title('Distribution of Days Lasted')\nplt.xlabel('Days Lasted')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRelationship between …\n\n\ngender vs days_lasted\n\nplt.figure(figsize = (8, 6))\nsns.boxplot(data = survival_df, x = 'gender', y = 'days_lasted', palette = 'bright')\nplt.title('Days Lasted vs Gender')\nplt.xlabel('Gender')\nplt.ylabel('Days Lasted')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nage vs days_lasted\n\nage_bins = pd.cut(survival_df['age'], bins = [20, 30, 40, 50, 60, 70], labels = [\"20-30\", \"30-40\", \"40-50\", \"50-60\", \"60-70\"])\n\nplt.figure(figsize = (8, 6))\nsns.boxplot(x = age_bins, y = survival_df['days_lasted'], palette = 'bright')\n# sns.boxplot(x = age_bins, y = survival_df['days_lasted'], palette = 'Paired')\nplt.title('Days Lasted vs Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Days Lasted')\nplt.show()"
  },
  {
    "objectID": "hw-04-manasppanse.html#task-2---data-cleaning-1-cr.",
    "href": "hw-04-manasppanse.html#task-2---data-cleaning-1-cr.",
    "title": "Regression Models",
    "section": "Task 2 - Data Cleaning (1 Cr.)",
    "text": "Task 2 - Data Cleaning (1 Cr.)\n\nHandling Missing Values\n\nsurvival_df.isnull().sum()\n\nseason                  0\nname                    0\nage                     0\ngender                  0\ncity                    0\nstate                   1\ncountry                 0\nresult                  0\ndays_lasted             0\nmedically_evacuated     0\nreason_tapped_out      10\nreason_category        10\nteam                   80\nday_linked_up          86\nprofession              0\nurl                     0\ndtype: int64\n\n\n\nfor col in survival_df.select_dtypes(include = ['int64', 'float64']):\n    survival_df[col].fillna(survival_df[col].median(), inplace = True)\n\nfor col in survival_df.select_dtypes(include = ['object', 'bool']):\n    survival_df[col].fillna(survival_df[col].mode()[0], inplace = True)\n\n\nsurvival_df.isnull().sum()\n\nseason                 0\nname                   0\nage                    0\ngender                 0\ncity                   0\nstate                  0\ncountry                0\nresult                 0\ndays_lasted            0\nmedically_evacuated    0\nreason_tapped_out      0\nreason_category        0\nteam                   0\nday_linked_up          0\nprofession             0\nurl                    0\ndtype: int64\n\n\n\n\nRemoving Outliers\n\n# Calculate IQR for 'days_lasted'\nQ1 = survival_df['days_lasted'].quantile(0.25)\nQ3 = survival_df['days_lasted'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Calculate bounds for detecting outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Filter out outliers\nclean_survival_df = survival_df[(survival_df['days_lasted'] &gt;= lower_bound) & (survival_df['days_lasted'] &lt;= upper_bound)]\n\n# Check how many rows were removed\nprint(f\"Before Rows : {len(survival_df)}\")\nprint(f\"Afterr Rows : {len(clean_survival_df)}\")\n\nBefore Rows : 94\nAfterr Rows : 94"
  },
  {
    "objectID": "hw-04-manasppanse.html#task-3---data-transformation-1-cr.",
    "href": "hw-04-manasppanse.html#task-3---data-transformation-1-cr.",
    "title": "Regression Models",
    "section": "Task 3 - Data Transformation (1 Cr.)",
    "text": "Task 3 - Data Transformation (1 Cr.)\n\nFeature Scaling\n\nscaler = StandardScaler()\n\nsurvival_df[['age', 'days_lasted']] = scaler.fit_transform(survival_df[['age', 'days_lasted']])\n\nsurvival_df[['age', 'days_lasted']].describe()\n\n\n\n\n\n\n\n\n\nage\ndays_lasted\n\n\n\n\ncount\n9.400000e+01\n9.400000e+01\n\n\nmean\n-8.976271e-17\n2.480285e-17\n\n\nstd\n1.005362e+00\n1.005362e+00\n\n\nmin\n-2.152391e+00\n-1.409434e+00\n\n\n25%\n-7.884039e-01\n-1.030384e+00\n\n\n50%\n6.408805e-02\n1.651380e-02\n\n\n75%\n6.892488e-01\n8.919373e-01\n\n\nmax\n2.621564e+00\n2.200560e+00\n\n\n\n\n\n\n\n\n\n\nVariable Transformation\n\nclean_survival_df['days_lasted_log'] = np.log1p(clean_survival_df['days_lasted'] + 1)\n\nclean_survival_df[['days_lasted', 'days_lasted_log']].head()\n\n\n\n\n\n\n\n\n\ndays_lasted\ndays_lasted_log\n\n\n\n\n0\n56\n4.060443\n\n\n1\n55\n4.043051\n\n\n2\n43\n3.806662\n\n\n3\n39\n3.713572\n\n\n4\n8\n2.302585\n\n\n\n\n\n\n\n\n\n\nHistogram of days_lasted_log\n\nplt.figure(figsize = (8, 6))\ndll = sns.histplot(clean_survival_df['days_lasted_log'], kde = True, bins = 20, color = 'lime')\ndll.lines[0].set_color('red')\nplt.title('Distribution of Log - Transformed Days Lasted')\nplt.xlabel('Log of Days Lasted')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe Remaining Processing Steps\n\n# Drop irrelevant columns\nsurvivalistsRed = clean_survival_df.drop(['name', 'city', 'url', 'profession', 'reason_tapped_out', 'state', 'country', 'reason_category', 'team', 'days_lasted'], axis = 1)\n\n# Label encoding for 'gender'\nsurvivalistsFinal = pd.get_dummies(survivalistsRed, columns = ['gender'], drop_first = True)\n\nsurvivalistsFinal['medically_evacuated'] = survivalistsFinal['medically_evacuated'].astype(int)\nsurvivalistsFinal['gender_Male'] = survivalistsFinal['gender_Male'].astype(int)\n\nsurvivalistsFinal.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 94 entries, 0 to 93\nData columns (total 7 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   season               94 non-null     int64  \n 1   age                  94 non-null     int64  \n 2   result               94 non-null     int64  \n 3   medically_evacuated  94 non-null     int32  \n 4   day_linked_up        94 non-null     float64\n 5   days_lasted_log      94 non-null     float64\n 6   gender_Male          94 non-null     int32  \ndtypes: float64(2), int32(2), int64(3)\nmemory usage: 4.5 KB"
  },
  {
    "objectID": "hw-04-manasppanse.html#model-building-1-cr.",
    "href": "hw-04-manasppanse.html#model-building-1-cr.",
    "title": "Regression Models",
    "section": "Model Building (1 Cr.)",
    "text": "Model Building (1 Cr.)\n\nx = survivalistsFinal.drop('days_lasted_log', axis = 1)\ny = survivalistsFinal['days_lasted_log']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\nx_train_const = sm.add_constant(x_train)\n\nols_model = sm.OLS(y_train, x_train_const).fit()\nols_model.summary()\n\n# NO IDEA WHY IT SHOWS THE TITLE 'OLS Regression Results' AFTER HALF OF THE RESULT !!!\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndays_lasted_log\nR-squared:\n0.715\n\n\nModel:\nOLS\nAdj. R-squared:\n0.689\n\n\nMethod:\nLeast Squares\nF-statistic:\n28.38\n\n\nDate:\nFri, 01 Nov 2024\nProb (F-statistic):\n1.01e-16\n\n\nTime:\n21:14:44\nLog-Likelihood:\n-60.591\n\n\nNo. Observations:\n75\nAIC:\n135.2\n\n\nDf Residuals:\n68\nBIC:\n151.4\n\n\nDf Model:\n6\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n0.2251\n2.650\n0.085\n0.933\n-5.063\n5.513\n\n\nseason\n0.1166\n0.026\n4.426\n0.000\n0.064\n0.169\n\n\nage\n0.0122\n0.007\n1.649\n0.104\n-0.003\n0.027\n\n\nresult\n-0.2645\n0.024\n-10.876\n0.000\n-0.313\n-0.216\n\n\nmedically_evacuated\n-0.2290\n0.167\n-1.373\n0.174\n-0.562\n0.104\n\n\nday_linked_up\n0.4193\n0.288\n1.456\n0.150\n-0.155\n0.994\n\n\ngender_Male\n-0.3012\n0.175\n-1.717\n0.091\n-0.651\n0.049\n\n\n\n\n\n\nOmnibus:\n2.326\nDurbin-Watson:\n1.878\n\n\nProb(Omnibus):\n0.313\nJarque-Bera (JB):\n2.035\n\n\nSkew:\n-0.403\nProb(JB):\n0.361\n\n\nKurtosis:\n2.962\nCond. No.\n1.65e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.65e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nInterpretation -\n\nseason & result are the most significant predictors. Later seasons increase the number of days lasted, while worse results decrease the number of days.\ngender shows some negative effect, which does suggest that The Boys may last fewer days, but the result isn’t that robust (statistically).\nage, medically_evaluated, and days_linked_up show some effects.\n\n\nModel Training and Evaluation (2 Cr.)\n\nx_test_const = sm.add_constant(x_test)\nx_test_const = x_test_const.reindex(columns = x_train_const.columns, fill_value = 0)\ny_pred = ols_model.predict(x_test_const)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error : {mse}\")\n\nr_squared = r2_score(y_test, y_pred)\nprint(f\"R-Squared : {r_squared}\")\n\nMean Squared Error : 0.47915396259842913\nR-Squared : 0.5011470534295956\n\n\nInterpretation -\n\nMSE : 0.479. This metric basically represents the average sq. difference between actual and predicted values. The lower this value, the better. In our case, there are still some errors in the predictions, although it might be a reasonable one given the log transformed scale of our days_lasted_log.\nR-Squared : 0.501. This metric basically explains how much was explained by the model. The higher this value, the better. In our case, our model captures half of the variability in the outcome based on the predictors."
  },
  {
    "objectID": "hw-04-manasppanse.html#task-6---residual-plots-1-cr.",
    "href": "hw-04-manasppanse.html#task-6---residual-plots-1-cr.",
    "title": "Regression Models",
    "section": "Task 6 - Residual Plots (1 Cr.)",
    "text": "Task 6 - Residual Plots (1 Cr.)\n\nresiduals = y_test - y_pred\n\n\nplt.figure(figsize = (8, 6))\nplt.scatter(y_pred, residuals, color = \"lime\")\nplt.axhline(y = 0, color = 'red', linestyle = '--')\nplt.title('Residual Plot')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.show()"
  },
  {
    "objectID": "hw-04-manasppanse.html#task-7---normality-of-residuals-1-cr.",
    "href": "hw-04-manasppanse.html#task-7---normality-of-residuals-1-cr.",
    "title": "Regression Models",
    "section": "Task 7 - Normality of Residuals (1 Cr.)",
    "text": "Task 7 - Normality of Residuals (1 Cr.)\n\nresiduals_train = ols_model.resid\n\nsm.qqplot(residuals_train, line = '45', fit = True)\nplt.title('Q - Q Plot of Residuals')\nplt.show()"
  },
  {
    "objectID": "hw-04-manasppanse.html#task-8---ridge-regression-2-cr.",
    "href": "hw-04-manasppanse.html#task-8---ridge-regression-2-cr.",
    "title": "Regression Models",
    "section": "Task 8 - Ridge Regression (2 Cr.)",
    "text": "Task 8 - Ridge Regression (2 Cr.)\n\nridge_cv = RidgeCV(alphas = alphas, store_cv_values = True)\nridge_cv.fit(x_train_scaled, y_train)\nbest_alpha_ridge = ridge_cv.alpha_\n\ny_pred_ridge = ridge_cv.predict(x_test_scaled)\nmse_ridge = mean_squared_error(y_test, y_pred_ridge)\nr2_ridge = r2_score(y_test, y_pred_ridge)\n\nprint(f\"Best λ (alpha)     : {best_alpha_ridge}\")\nprint(f\"Mean Squared Error : {mse_ridge}\")\nprint(f\"R-Squared          : {r2_ridge}\")\n\nBest λ (alpha)     : 1.0\nMean Squared Error : 0.43606391143638773\nR-Squared          : 0.5460086233381156"
  },
  {
    "objectID": "hw-04-manasppanse.html#opt-task---lasso-regression",
    "href": "hw-04-manasppanse.html#opt-task---lasso-regression",
    "title": "Regression Models",
    "section": "OPT Task - Lasso Regression",
    "text": "OPT Task - Lasso Regression\n\nlasso_cv = LassoCV(alphas = alphas, cv = 5)\nlasso_cv.fit(x_train_scaled, y_train)\nbest_alpha_lasso = lasso_cv.alpha_\n\ny_pred_lasso = lasso_cv.predict(x_test_scaled)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nr2_lasso = r2_score(y_test, y_pred_lasso)\n\nprint(f\"Best λ (alpha)     : {best_alpha_lasso}\")\nprint(f\"Mean Squared Error : {mse_lasso}\")\nprint(f\"R-Squared          : {r2_lasso}\")\n\nBest λ (alpha)     : 0.1\nMean Squared Error : 0.4323847164869538\nR-Squared          : 0.5498390773983908"
  },
  {
    "objectID": "hw-02-manasppanse.html",
    "href": "hw-02-manasppanse.html",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "",
    "text": "# Checking Python Version\n\n!python --version\n\nPython 3.12.3\n\n\n\n# Importing Necessary Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport scipy.stats as stats\n\n\n# Importing Dataset\n\nsongs_df = pd.read_csv(\"data/hw-02/taylor_album_songs.csv\")"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-1---data-overview-1-cr.",
    "href": "hw-02-manasppanse.html#task-1---data-overview-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 1 - Data Overview (1 Cr.)",
    "text": "Task 1 - Data Overview (1 Cr.)\n\nsongs_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 194 entries, 0 to 193\nData columns (total 29 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   album_name           194 non-null    object \n 1   ep                   194 non-null    bool   \n 2   album_release        194 non-null    object \n 3   track_number         194 non-null    int64  \n 4   track_name           194 non-null    object \n 5   artist               191 non-null    object \n 6   featuring            16 non-null     object \n 7   bonus_track          194 non-null    bool   \n 8   promotional_release  18 non-null     object \n 9   single_release       36 non-null     object \n 10  track_release        194 non-null    object \n 11  danceability         191 non-null    float64\n 12  energy               191 non-null    float64\n 13  key                  191 non-null    float64\n 14  loudness             191 non-null    float64\n 15  mode                 191 non-null    float64\n 16  speechiness          191 non-null    float64\n 17  acousticness         191 non-null    float64\n 18  instrumentalness     191 non-null    float64\n 19  liveness             191 non-null    float64\n 20  valence              191 non-null    float64\n 21  tempo                191 non-null    float64\n 22  time_signature       191 non-null    float64\n 23  duration_ms          191 non-null    float64\n 24  explicit             191 non-null    object \n 25  key_name             191 non-null    object \n 26  mode_name            191 non-null    object \n 27  key_mode             191 non-null    object \n 28  lyrics               0 non-null      float64\ndtypes: bool(2), float64(14), int64(1), object(12)\nmemory usage: 41.4+ KB\n\n\n\nShape\nThe dataset contains 194 ROWS and 29 COLUMNS.\n\n\nColumns\n\nCategorical Columns : album_name, ep, album_release, track_name, artist, featuring, bonus_track, promotional_release, single_release, track_release, explicit, key_name, mode_name, key_mode.\nNumerical Columns : track_number, danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, time_signature, duration_ms, lyrics.\n\n\n\nDataTypes\n\nobject : 12 columns.\nbool : 02 columns.\nint64 : 01 column.\nfloat64 : 14 columns.\n\n\n\nDescriptive Statistics\n\nsongs_df.describe()\n\n\n\n\n\n\n\n\n\ntrack_number\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nduration_ms\nlyrics\n\n\n\n\ncount\n194.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n191.000000\n0.0\n\n\nmean\n10.706186\n0.584016\n0.574492\n4.685864\n-7.518058\n0.910995\n0.058310\n0.321225\n0.003936\n0.140813\n0.400857\n125.991424\n3.979058\n237078.942408\nNaN\n\n\nstd\n6.599675\n0.115372\n0.183443\n3.342826\n2.775821\n0.285500\n0.056866\n0.329021\n0.030607\n0.079951\n0.190456\n31.959403\n0.289430\n47316.125333\nNaN\n\n\nmin\n1.000000\n0.292000\n0.131000\n0.000000\n-15.434000\n0.000000\n0.023100\n0.000191\n0.000000\n0.035700\n0.038200\n68.534000\n1.000000\n148781.000000\nNaN\n\n\n25%\n5.000000\n0.511000\n0.446500\n2.000000\n-9.326000\n1.000000\n0.030800\n0.034600\n0.000000\n0.092950\n0.253500\n99.978000\n4.000000\n209326.500000\nNaN\n\n\n50%\n10.000000\n0.594000\n0.580000\n5.000000\n-6.937000\n1.000000\n0.039600\n0.162000\n0.000001\n0.115000\n0.404000\n121.956000\n4.000000\n232107.000000\nNaN\n\n\n75%\n15.000000\n0.652000\n0.717000\n7.000000\n-5.606000\n1.000000\n0.057400\n0.662000\n0.000040\n0.150500\n0.534500\n150.027500\n4.000000\n254447.500000\nNaN\n\n\nmax\n30.000000\n0.897000\n0.950000\n11.000000\n-2.098000\n1.000000\n0.519000\n0.971000\n0.348000\n0.594000\n0.942000\n208.918000\n5.000000\n613027.000000\nNaN"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-2---univariate-analysis-1-cr.",
    "href": "hw-02-manasppanse.html#task-2---univariate-analysis-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 2 - Univariate Analysis (1 Cr.)",
    "text": "Task 2 - Univariate Analysis (1 Cr.)\n\n# Numerical Columns\nnumeric_cols = songs_df.select_dtypes(include=['float64', 'int64']).columns\n\n# Categorical Columns\ncategoric_cols = songs_df.select_dtypes(include=['object', 'bool']).columns\n\n\nNumeric Columns Plot\n\nplt.figure(figsize = (15, 12))\nfor i, column in enumerate(numeric_cols, 1):\n  plt.subplot(4, 4, i)\n  sns.histplot(songs_df[column], kde = True)\n  plt.title(f'Histogram of {column}')\n  plt.xlabel(column)\n  plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCategoric Columns Plot\n\nplt.figure(figsize = (15, 12))\nfor i, column in enumerate(categoric_cols, 1):\n  plt.subplot(4, 4, i)\n  sns.histplot(songs_df[column], kde = True)\n  plt.title(f'Histogram of {column}')\n  plt.xlabel(column)\n  plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-3---bivariate-analysis-1-cr.",
    "href": "hw-02-manasppanse.html#task-3---bivariate-analysis-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 3 - Bivariate Analysis (1 Cr.)",
    "text": "Task 3 - Bivariate Analysis (1 Cr.)\n\nDanceability vs Energy\n\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = \"danceability\", y = \"energy\", data = songs_df, hue = \"album_name\")\nplt.title(\"Danceability vs Energy\", pad = 50)\nplt.legend(title = \"Album\", loc = 'upper center', bbox_to_anchor = (0.5, 1.15), fontsize = \"small\", ncol = 5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLoudness vs Acousticness\n\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = \"loudness\", y = \"acousticness\", data = songs_df, hue = \"album_name\")\nplt.title(\"Loudness vs Acousticness\", pad = 50)\nplt.legend(title = \"Album\", loc = 'upper center', bbox_to_anchor = (0.5, 1.15), fontsize = \"small\", ncol = 5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nValence vs Tempo\n\nplt.figure(figsize = (8, 6))\nsns.scatterplot(x = \"valence\", y = \"tempo\", data = songs_df, hue = \"album_name\")\nplt.title(\"Valence vs Tempo\", pad = 50)\nplt.legend(title = \"Album\", loc = 'upper center', bbox_to_anchor = (0.5, 1.15), fontsize = \"small\", ncol = 5)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-4---missing-data-outliers-1-cr.",
    "href": "hw-02-manasppanse.html#task-4---missing-data-outliers-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 4 - Missing Data & Outliers (1 Cr.)",
    "text": "Task 4 - Missing Data & Outliers (1 Cr.)\n\nNull Values\n\nsongs_df.isnull().sum()\n\nalbum_name               0\nep                       0\nalbum_release            0\ntrack_number             0\ntrack_name               0\nartist                   3\nfeaturing              178\nbonus_track              0\npromotional_release    176\nsingle_release         158\ntrack_release            0\ndanceability             3\nenergy                   3\nkey                      3\nloudness                 3\nmode                     3\nspeechiness              3\nacousticness             3\ninstrumentalness         3\nliveness                 3\nvalence                  3\ntempo                    3\ntime_signature           3\nduration_ms              3\nexplicit                 3\nkey_name                 3\nmode_name                3\nkey_mode                 3\nlyrics                 194\ndtype: int64\n\n\nColumns with NULLs : artist, featuring, promotional_release, single_release, danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, time_signature, duration_ms, lyrics.\n\n\nOutlier Detection\n\ndef find_outliers(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    return df[(df[column] &lt; lower_bound) | (df[column] &gt; upper_bound)]\n\noutliers_count = {col: len(find_outliers(songs_df, col)) for col in numeric_cols}\nprint(\"Total Outliers for each Numerical Column :\\n\", outliers_count)\n\nTotal Outliers for each Numerical Column :\n {'track_number': 0, 'danceability': 3, 'energy': 0, 'key': 0, 'loudness': 2, 'mode': 17, 'speechiness': 20, 'acousticness': 0, 'instrumentalness': 40, 'liveness': 21, 'valence': 0, 'tempo': 0, 'time_signature': 8, 'duration_ms': 5, 'lyrics': 0}"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-5---handling-missing-values-1-cr.",
    "href": "hw-02-manasppanse.html#task-5---handling-missing-values-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 5 - Handling Missing Values (1 Cr.)",
    "text": "Task 5 - Handling Missing Values (1 Cr.)\n\nfor col in numeric_cols:\n    songs_df[col].fillna(songs_df[col].median(), inplace = True)\n\n\nfor col in categoric_cols:\n    songs_df[col].fillna(songs_df[col].mode().iloc[0], inplace = True)\n\n\nsongs_df.isnull().sum()\n\nalbum_name               0\nep                       0\nalbum_release            0\ntrack_number             0\ntrack_name               0\nartist                   0\nfeaturing                0\nbonus_track              0\npromotional_release      0\nsingle_release           0\ntrack_release            0\ndanceability             0\nenergy                   0\nkey                      0\nloudness                 0\nmode                     0\nspeechiness              0\nacousticness             0\ninstrumentalness         0\nliveness                 0\nvalence                  0\ntempo                    0\ntime_signature           0\nduration_ms              0\nexplicit                 0\nkey_name                 0\nmode_name                0\nkey_mode                 0\nlyrics                 194\ndtype: int64"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-6---dealing-with-outliers-1-cr.",
    "href": "hw-02-manasppanse.html#task-6---dealing-with-outliers-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 6 - Dealing with Outliers (1 Cr.)",
    "text": "Task 6 - Dealing with Outliers (1 Cr.)\n\nplt.figure(figsize = (8, 6))\nsns.kdeplot(songs_df['danceability'], shade = True)\nplt.title('Density Plot of Danceability Before Capping')\nplt.show()\n\n\n\n\n\n\n\n\n\ntask6_lb = songs_df['danceability'].quantile(0.10) # Cutoff at 10th\ntask6_ub = songs_df['danceability'].quantile(0.90) # Cutoff at 90th\n\nsongs_df['danceability'] = songs_df['danceability'].clip(lower = task6_lb, upper = task6_ub)\n\n# Visualizing the Effect of Capping\nplt.figure(figsize = (8, 6))\nsns.kdeplot(songs_df['danceability'], shade = True)\nplt.title('Density Plot of Danceability After Capping')\nplt.show()"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-7---feature-engineering-1-cr.",
    "href": "hw-02-manasppanse.html#task-7---feature-engineering-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 7 - Feature Engineering (1 Cr.)",
    "text": "Task 7 - Feature Engineering (1 Cr.)\n\nsongs_df['valence_tempo_ratio'] = songs_df['valence'] / songs_df['tempo']\n\nprint(songs_df[['track_name', 'valence', 'tempo', 'valence_tempo_ratio']].head())\n\n               track_name  valence    tempo  valence_tempo_ratio\n0              Tim McGraw    0.425   76.009             0.005591\n1         Picture To Burn    0.821  105.586             0.007776\n2  Teardrops On My Guitar    0.289   99.953             0.002891\n3   A Place In This World    0.428  115.028             0.003721\n4             Cold As You    0.261  175.558             0.001487"
  },
  {
    "objectID": "hw-02-manasppanse.html#task-8---data-transformation-1-cr.",
    "href": "hw-02-manasppanse.html#task-8---data-transformation-1-cr.",
    "title": "Exploratory Data Analysis & Data Pre-Processing",
    "section": "Task 8 - Data Transformation (1 Cr.)",
    "text": "Task 8 - Data Transformation (1 Cr.)\n\nZ - Score Normalization\n\n# Z-Score Normalization\nscaler = StandardScaler()\nsongs_df['tempo_zscore'] = scaler.fit_transform(songs_df[['tempo']])\n\n# Transformation Check\nsongs_df['tempo_zscore'].describe()\n\ncount    1.940000e+02\nmean    -5.951711e-16\nstd      1.002587e+00\nmin     -1.814453e+00\n25%     -8.194760e-01\n50%     -1.256008e-01\n75%      7.578285e-01\nmax      2.623566e+00\nName: tempo_zscore, dtype: float64\n\n\n\n\nTransformation for Skewness\n\n# Density Plot\nplt.figure(figsize = (8, 6))\nsns.kdeplot(songs_df['loudness'], shade = True)\nplt.title('Density Plot of Loudness Before Transformation')\nplt.show()\n\n# Q-Q Plot\nplt.figure(figsize = (8, 6))\nstats.probplot(songs_df['loudness'], dist = \"norm\", plot = plt)\nplt.title('Q-Q Plot of Loudness Before Transformation')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Applying Log Transformation to Correct Skewness\nsongs_df['loudness_log'] = np.log1p(songs_df['loudness'] - songs_df['loudness'].min() + 1)\n\n\n# Density Plot\nplt.figure(figsize = (8, 6))\nsns.kdeplot(songs_df['loudness_log'], shade = True)\nplt.title('Density Plot of Loudness (After Log Transformation)')\nplt.show()\n\n# Q-Q Plot\nplt.figure(figsize = (8, 6))\nstats.probplot(songs_df['loudness_log'], dist = \"norm\", plot = plt)\nplt.title('Q-Q Plot of Loudness (After Log Transformation)')\nplt.show()"
  },
  {
    "objectID": "hw-01-manasppanse.html",
    "href": "hw-01-manasppanse.html",
    "title": "Python & NumPy Basics",
    "section": "",
    "text": "# Checking Python Version\n\n!python --version\n\nPython 3.12.3"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-1---variables-type-1-cr.",
    "href": "hw-01-manasppanse.html#task-1---variables-type-1-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 1 - Variables & Type (1 Cr.)",
    "text": "Task 1 - Variables & Type (1 Cr.)\n\nDefine two variables: an integer named age with a value of 25 and a string named course with the value “Data Mining”.\nPrint their values and types using the print() and the type() function.\n\n\n# Defining Variables\nage = 25\ncourse = \"Data Mining\"\n\n# Printing Values & Types\nprint(\"Value of 'age' Variable :\", age)\nprint(\"Type of 'age' Variable :\", type(age))\n\nprint(\"\\nValue of 'course' Variable :\", course)\nprint(\"Type of 'course' Variable :\", type(course))\n\nValue of 'age' Variable : 25\nType of 'age' Variable : &lt;class 'int'&gt;\n\nValue of 'course' Variable : Data Mining\nType of 'course' Variable : &lt;class 'str'&gt;"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-2---control-structures-2-cr.",
    "href": "hw-01-manasppanse.html#task-2---control-structures-2-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 2 - Control Structures (2 Cr.)",
    "text": "Task 2 - Control Structures (2 Cr.)\n\nWrite a function is_prime(num) that takes an integer and returns True if the number is a prime number, False otherwise.\nMake sure you include a loop and an appropriate control flow statement to check for primality.\n\n\n# Defining the Function\ndef is_prime(num):\n    if num &lt;= 1:\n        return False\n    for i in range(2, int(num ** 0.5) + 1):\n        if num % i == 0:\n            return False\n    return True\n\n# Testing\nprint(is_prime(1))\nprint(is_prime(2))\n\nFalse\nTrue"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-3---data-structures-2-cr.",
    "href": "hw-01-manasppanse.html#task-3---data-structures-2-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 3 - Data Structures (2 Cr.)",
    "text": "Task 3 - Data Structures (2 Cr.)\n\nCreate a dictionary named student_grades with keys as student names and values as their grades (A, B, C, D, F).\nWrite a loop to print out each student’s name and grade in the format: “Student [Name] has grade [Grade]”.\n\n\n# Creating a Dictionary named student_grades\nstudent_grades = {\n    'Manas': 'A',\n    'Bob': 'B',\n    'Jack': 'C',\n    'Mark': 'D',\n    'Wade': 'F'\n}\n\n# Looping a Print statement in the given format\nfor name, grade in student_grades.items():\n    print(f\"Student {name} has grade {grade}\")\n\nStudent Manas has grade A\nStudent Bob has grade B\nStudent Jack has grade C\nStudent Mark has grade D\nStudent Wade has grade F"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-4---numpy-arrays-1-cr.",
    "href": "hw-01-manasppanse.html#task-4---numpy-arrays-1-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 4 - NumPy Arrays (1 Cr.)",
    "text": "Task 4 - NumPy Arrays (1 Cr.)\n\nCreate a NumPy array A of shape (10,10) with values ranging from 0 to 99.\nCalculate the determinant of matrix A (use numpy.linalg.det). Print the result.\n\n\n# Creating the NumPy array `A`\nA = [\n  [47, 25,  8, 14, 61, 30, 87, 56,  9, 92],\n  [ 7, 66, 95, 42, 34, 77, 18, 54,  0, 81],\n  [38, 73, 64, 25,  5, 90, 16, 43, 57, 28],\n  [21, 50,  3, 87, 64, 29, 79, 94, 67, 41],\n  [ 4, 22, 18, 31, 96, 45, 72, 35, 60,  7],\n  [89, 62,  1, 74, 13, 68, 88, 27, 50, 12],\n  [76, 82,  5, 49, 53, 85, 33,  4, 24, 97],\n  [31,  6, 92, 78,  9, 46, 70, 19,  2, 83],\n  [65, 93, 28, 71, 40, 56,  7, 95, 82, 19],\n  [44, 91, 13, 52, 59, 37, 48,  6, 20, 85]\n]\n\n# Calculating the Determinant\ndeterminant = np.linalg.det(A)\n\n# Printing Result\nprint(f\"Determinant of the matrix A ( Δ ) : {determinant}\")\n\nDeterminant of the matrix A ( Δ ) : 1.0088849280721224e+19"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-5---iterate-over-elements-2-cr.",
    "href": "hw-01-manasppanse.html#task-5---iterate-over-elements-2-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 5 - Iterate over Elements (2 Cr.)",
    "text": "Task 5 - Iterate over Elements (2 Cr.)\n\nTract and print all the elements from the third column of a given 2D NumPy array.\nUse a for loop to iterate through each element of this column and print their square roots.\n\nGiven Code -\nimport numpy as np\n\n# Assuming a 2D array example\narray_2D = np.random.randint(1, 100, size=(5, 5))\n\n# complete the rest of codes here...\n\n# Creating a 2D Array\narray_2D = np.random.randint(1, 100, size=(5, 5))\n\n# Printing the Original Array\nprint(\"Original 2D Array : \")\nprint(array_2D)\n\n# Extracting the 3rd Column\nthird_column = array_2D[:, 2]\n\n# Printing the 3rd Column\nprint(\"\\nThird Column of the Array :\")\nprint(third_column)\n\n# Iterating through each Element of the 3rd Column and Printing their Square Roots\nprint(\"\\nSquare Roots of the Third Column Elements :\")\nfor element in third_column:\n    print(np.sqrt(element))\n\nOriginal 2D Array : \n[[81 55 16 49 94]\n [19 60 46 77 40]\n [63 49 30 95 21]\n [81  9 57 34 74]\n [84 10 95 63 93]]\n\nThird Column of the Array :\n[16 46 30 57 95]\n\nSquare Roots of the Third Column Elements :\n4.0\n6.782329983125268\n5.477225575051661\n7.54983443527075\n9.746794344808963"
  },
  {
    "objectID": "hw-01-manasppanse.html#task-6-statistics-with-numpy-2-cr.",
    "href": "hw-01-manasppanse.html#task-6-statistics-with-numpy-2-cr.",
    "title": "Python & NumPy Basics",
    "section": "Task 6: Statistics with NumPy (2 Cr.)",
    "text": "Task 6: Statistics with NumPy (2 Cr.)\n\nGiven a 2D NumPy array, calculate the mean, median, and variance along both rows and columns.\nIdentify the row with the maximum variance and print it out.\n\n\n# Saving Timg and Borrowing the original 2D array from TASK 5\nprint(\"Original 2D Array :\")\nprint(array_2D)\n\nOriginal 2D Array :\n[[81 55 16 49 94]\n [19 60 46 77 40]\n [63 49 30 95 21]\n [81  9 57 34 74]\n [84 10 95 63 93]]\n\n\n\n# Calculating Mean along Columns (axis = 0)\nmean_columns = np.mean(array_2D, axis=0)\nprint(\"Mean along Columns :\")\nprint(mean_columns)\n\n# Calculating Mean along Rows (axis = 1)\nmean_rows = np.mean(array_2D, axis=1)\nprint(\"\\nMean along Rows :\")\nprint(mean_rows)\n\nMean along Columns :\n[65.6 36.6 48.8 63.6 64.4]\n\nMean along Rows :\n[59.  48.4 51.6 51.  69. ]\n\n\n\n# Median along Columns (axis = 0)\nmedian_columns = np.median(array_2D, axis=0)\nprint(\"Median along Columns :\")\nprint(median_columns)\n\n# Median along Rows (axis = 1)\nmedian_rows = np.median(array_2D, axis=1)\nprint(\"\\nMedian along Rows :\")\nprint(median_rows)\n\nMedian along Columns :\n[81. 49. 46. 63. 74.]\n\nMedian along Rows :\n[55. 46. 49. 57. 84.]\n\n\n\n# Variance along columns (axis=0)\nvariance_columns = np.var(array_2D, axis=0)\nprint(\"Variance along Columns :\")\nprint(variance_columns)\n\n# Variance along rows (axis=1)\nvariance_rows = np.var(array_2D, axis=1)\nprint(\"\\nVariance along Rows :\")\nprint(variance_rows)\n\nVariance along Columns :\n[598.24 501.84 727.76 451.04 853.04]\n\nVariance along Rows :\n[734.8  378.64 684.64 703.6  998.8 ]\n\n\n\n# Identifying the Row with the MAXIMUM Variance\nmax_variance_row_index = np.argmax(variance_rows)\nmax_variance_row = array_2D[max_variance_row_index]\n\n# Print the Row with the MAXIMUM Variance\nprint(\"Row with the MAX Variance :\")\nprint(max_variance_row)\n\nRow with the MAX Variance :\n[84 10 95 63 93]"
  },
  {
    "objectID": "hw-03-manasppanse.html",
    "href": "hw-03-manasppanse.html",
    "title": "Classification & Model Evaluation",
    "section": "",
    "text": "# Checking Python Version\n\n!python --version\n\nPython 3.12.3\n\n\n\n# Importing Necessary Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score, make_scorer\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score\nfrom sklearn.model_selection import cross_validate, cross_val_score, StratifiedKFold, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\n\n# Importing Dataset\n\ncosts_df = pd.read_csv(\"data/hw-03/childcare_costs.csv\")"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-1---exploratory-data-analysis-1-cr.",
    "href": "hw-03-manasppanse.html#task-1---exploratory-data-analysis-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 1 - Exploratory Data Analysis (1 Cr.)",
    "text": "Task 1 - Exploratory Data Analysis (1 Cr.)\n\nData Overview\n\ncosts_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 23342 entries, 0 to 23341\nData columns (total 64 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   county_fips_code           23342 non-null  int64  \n 1   study_year                 23342 non-null  int64  \n 2   unr_16                     23342 non-null  float64\n 3   funr_16                    23342 non-null  float64\n 4   munr_16                    23342 non-null  float64\n 5   unr_20to64                 23342 non-null  float64\n 6   funr_20to64                23342 non-null  float64\n 7   munr_20to64                23342 non-null  float64\n 8   flfpr_20to64               23342 non-null  float64\n 9   flfpr_20to64_under6        23342 non-null  float64\n 10  flfpr_20to64_6to17         23342 non-null  float64\n 11  flfpr_20to64_under6_6to17  23342 non-null  float64\n 12  mlfpr_20to64               23342 non-null  float64\n 13  pr_f                       23342 non-null  float64\n 14  pr_p                       23342 non-null  float64\n 15  mhi_2018                   23342 non-null  float64\n 16  me_2018                    23342 non-null  float64\n 17  fme_2018                   23342 non-null  float64\n 18  mme_2018                   23342 non-null  float64\n 19  total_pop                  23342 non-null  int64  \n 20  one_race                   23342 non-null  float64\n 21  one_race_w                 23342 non-null  float64\n 22  one_race_b                 23342 non-null  float64\n 23  one_race_i                 23342 non-null  float64\n 24  one_race_a                 23342 non-null  float64\n 25  one_race_h                 23342 non-null  float64\n 26  one_race_other             23342 non-null  float64\n 27  two_races                  23342 non-null  float64\n 28  hispanic                   23342 non-null  float64\n 29  households                 23342 non-null  int64  \n 30  h_under6_both_work         23342 non-null  int64  \n 31  h_under6_f_work            23342 non-null  int64  \n 32  h_under6_m_work            23342 non-null  int64  \n 33  h_under6_single_m          23342 non-null  float64\n 34  h_6to17_both_work          23342 non-null  int64  \n 35  h_6to17_fwork              23342 non-null  int64  \n 36  h_6to17_mwork              23342 non-null  int64  \n 37  h_6to17_single_m           23342 non-null  float64\n 38  emp_m                      23342 non-null  float64\n 39  memp_m                     23342 non-null  float64\n 40  femp_m                     23342 non-null  float64\n 41  emp_service                23342 non-null  float64\n 42  memp_service               23342 non-null  float64\n 43  femp_service               23342 non-null  float64\n 44  emp_sales                  23342 non-null  float64\n 45  memp_sales                 23342 non-null  float64\n 46  femp_sales                 23342 non-null  float64\n 47  emp_n                      23342 non-null  float64\n 48  memp_n                     23342 non-null  float64\n 49  femp_n                     23342 non-null  float64\n 50  emp_p                      23342 non-null  float64\n 51  memp_p                     23342 non-null  float64\n 52  femp_p                     23342 non-null  float64\n 53  mcsa                       23342 non-null  float64\n 54  mfccsa                     23342 non-null  float64\n 55  mc_infant                  23342 non-null  float64\n 56  mc_toddler                 23342 non-null  float64\n 57  mc_preschool               23342 non-null  float64\n 58  mfcc_infant                23342 non-null  float64\n 59  mfcc_toddler               23342 non-null  float64\n 60  mfcc_preschool             23342 non-null  float64\n 61  county_name                23342 non-null  object \n 62  state_name                 23342 non-null  object \n 63  state_abbreviation         23342 non-null  object \ndtypes: float64(51), int64(10), object(3)\nmemory usage: 11.4+ MB\n\n\n\nShape\nThe dataset contains 23341 ROWS and 64 COLUMNS.\n\n\nColumns\n\nCategorical Columns : country_name, state_name, state_abbreviation.\nNumerical Columns : county_fips_code, study_year, one_race, etc. ( I am not going to type out all those columns. )\n\n\n\nDataTypes\n\nint64 : 10 column.\nfloat64 : 51 columns.\nobject : 03 columns.\n\n\n\n\nDescriptive Statistics\n\ncosts_df.describe()\n\n\n\n\n\n\n\n\n\ncounty_fips_code\nstudy_year\nunr_16\nfunr_16\nmunr_16\nunr_20to64\nfunr_20to64\nmunr_20to64\nflfpr_20to64\nflfpr_20to64_under6\n...\nmemp_p\nfemp_p\nmcsa\nmfccsa\nmc_infant\nmc_toddler\nmc_preschool\nmfcc_infant\nmfcc_toddler\nmfcc_preschool\n\n\n\n\ncount\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n...\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n23342.000000\n\n\nmean\n33273.162411\n2013.422286\n7.521682\n7.061460\n7.939197\n6.962311\n6.510894\n7.369356\n69.557325\n68.569269\n...\n23.522334\n7.360524\n101.194718\n92.469411\n145.695372\n130.094115\n121.927485\n113.388567\n106.723178\n104.153133\n\n\nstd\n15581.597214\n3.080393\n3.550907\n3.590273\n4.036159\n3.462389\n3.504860\n3.995685\n7.880718\n11.904121\n...\n7.657839\n4.249184\n34.573263\n27.643789\n53.554511\n43.477543\n38.336487\n32.825649\n29.982399\n28.957938\n\n\nmin\n1001.000000\n2008.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n35.100000\n0.000000\n...\n0.000000\n0.000000\n20.190000\n22.000000\n36.300000\n33.000000\n33.000000\n43.080000\n43.080000\n40.030000\n\n\n25%\n21043.000000\n2011.000000\n5.180000\n4.690000\n5.272500\n4.700000\n4.200000\n4.772500\n64.500000\n62.200000\n...\n17.880000\n4.340000\n78.490000\n75.000000\n108.570000\n100.000000\n95.830000\n90.000000\n85.000000\n84.212500\n\n\n50%\n38005.000000\n2014.000000\n7.130000\n6.630000\n7.480000\n6.570000\n6.100000\n6.900000\n70.000000\n69.400000\n...\n23.450000\n6.390000\n96.500000\n88.050000\n133.835000\n120.665000\n113.980000\n106.000000\n100.090000\n99.650000\n\n\n75%\n48088.500000\n2016.000000\n9.420000\n8.900000\n10.040000\n8.800000\n8.300000\n9.400000\n75.100000\n75.900000\n...\n28.820000\n9.510000\n119.337500\n107.480000\n165.010000\n147.630000\n138.095000\n129.250000\n124.620000\n120.000000\n\n\nmax\n56045.000000\n2018.000000\n30.930000\n38.240000\n39.740000\n33.600000\n44.500000\n45.500000\n100.000000\n100.000000\n...\n87.500000\n39.260000\n375.400000\n308.000000\n470.000000\n419.000000\n385.000000\n430.940000\n376.320000\n331.340000\n\n\n\n\n8 rows × 61 columns\n\n\n\n\n\n\nColumn Separations for Future Use\n\n# Numerical Columns\nnumeric_cols = costs_df.select_dtypes(include = ['float64', 'int64']).columns\n\n# Categorical Columns\ncategoric_cols = costs_df.select_dtypes(include = ['object']).columns"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-2---data-preprocessing-1-cr.",
    "href": "hw-03-manasppanse.html#task-2---data-preprocessing-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 2 - Data Preprocessing (1 Cr.)",
    "text": "Task 2 - Data Preprocessing (1 Cr.)\n\nLabel Encoding\n\nfor col in categoric_cols:\n    le = LabelEncoder()\n    costs_df[col] = le.fit_transform(costs_df[col])\n\ncosts_df.head()\n\n\n\n\n\n\n\n\n\ncounty_fips_code\nstudy_year\nunr_16\nfunr_16\nmunr_16\nunr_20to64\nfunr_20to64\nmunr_20to64\nflfpr_20to64\nflfpr_20to64_under6\n...\nmfccsa\nmc_infant\nmc_toddler\nmc_preschool\nmfcc_infant\nmfcc_toddler\nmfcc_preschool\ncounty_name\nstate_name\nstate_abbreviation\n\n\n\n\n0\n1001\n2008\n5.42\n4.41\n6.32\n4.6\n3.5\n5.6\n68.9\n66.9\n...\n81.40\n104.95\n104.95\n85.92\n83.45\n83.45\n81.40\n78\n0\n1\n\n\n1\n1001\n2009\n5.93\n5.72\n6.11\n4.8\n4.6\n5.0\n70.8\n63.7\n...\n85.68\n105.11\n105.11\n87.59\n87.39\n87.39\n85.68\n78\n0\n1\n\n\n2\n1001\n2010\n6.21\n5.57\n6.78\n5.1\n4.6\n5.6\n71.3\n67.0\n...\n89.96\n105.28\n105.28\n89.26\n91.33\n91.33\n89.96\n78\n0\n1\n\n\n3\n1001\n2011\n7.55\n8.13\n7.03\n6.2\n6.3\n6.1\n70.2\n66.5\n...\n94.25\n105.45\n105.45\n90.93\n95.28\n95.28\n94.25\n78\n0\n1\n\n\n4\n1001\n2012\n8.60\n8.88\n8.29\n6.7\n6.4\n7.0\n70.6\n67.1\n...\n98.53\n105.61\n105.61\n92.60\n99.22\n99.22\n98.53\n78\n0\n1\n\n\n\n\n5 rows × 64 columns"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-3---feature-engineering-1-cr.",
    "href": "hw-03-manasppanse.html#task-3---feature-engineering-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 3 - Feature Engineering (1 Cr.)",
    "text": "Task 3 - Feature Engineering (1 Cr.)\n\n# Grouping DF by 'state_name' & Calculating the Average 'mc_preschool' for Each State\nstate_avg = costs_df.groupby('state_name')['mc_preschool'].mean().reset_index()\nstate_avg.columns = ['state_name', 'avg_mc_preschool']\n\n# Merging the 'state_avg' with the OG DF\ncosts_df = costs_df.merge(state_avg, on = 'state_name')\n\n# Creating the 'above_state_avg' column (CONDITION : 1 if 'mc_preschool' is above 'state_avg', OTHERWISE 0\ncosts_df['above_state_avg'] = costs_df['mc_preschool'] &gt; costs_df['avg_mc_preschool'].astype(int)\n\n# Dropping the now unneccessary 'avg_mc_preschool' column\ncosts_df = costs_df.drop(columns = ['avg_mc_preschool'])\n\ncosts_df.head()\n\n\n\n\n\n\n\n\n\ncounty_fips_code\nstudy_year\nunr_16\nfunr_16\nmunr_16\nunr_20to64\nfunr_20to64\nmunr_20to64\nflfpr_20to64\nflfpr_20to64_under6\n...\nmc_infant\nmc_toddler\nmc_preschool\nmfcc_infant\nmfcc_toddler\nmfcc_preschool\ncounty_name\nstate_name\nstate_abbreviation\nabove_state_avg\n\n\n\n\n0\n1001\n2008\n5.42\n4.41\n6.32\n4.6\n3.5\n5.6\n68.9\n66.9\n...\n104.95\n104.95\n85.92\n83.45\n83.45\n81.40\n78\n0\n1\nFalse\n\n\n1\n1001\n2009\n5.93\n5.72\n6.11\n4.8\n4.6\n5.0\n70.8\n63.7\n...\n105.11\n105.11\n87.59\n87.39\n87.39\n85.68\n78\n0\n1\nFalse\n\n\n2\n1001\n2010\n6.21\n5.57\n6.78\n5.1\n4.6\n5.6\n71.3\n67.0\n...\n105.28\n105.28\n89.26\n91.33\n91.33\n89.96\n78\n0\n1\nFalse\n\n\n3\n1001\n2011\n7.55\n8.13\n7.03\n6.2\n6.3\n6.1\n70.2\n66.5\n...\n105.45\n105.45\n90.93\n95.28\n95.28\n94.25\n78\n0\n1\nFalse\n\n\n4\n1001\n2012\n8.60\n8.88\n8.29\n6.7\n6.4\n7.0\n70.6\n67.1\n...\n105.61\n105.61\n92.60\n99.22\n99.22\n98.53\n78\n0\n1\nFalse\n\n\n\n\n5 rows × 65 columns"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-4---feature-selection-1-cr.",
    "href": "hw-03-manasppanse.html#task-4---feature-selection-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 4 - Feature Selection (1 Cr.)",
    "text": "Task 4 - Feature Selection (1 Cr.)\n\ncorr_matrix = costs_df[numeric_cols].corr()\n\n\nHeatmap of Numeric Features\n\nplt.figure(figsize = (8, 6))\nsns.heatmap(corr_matrix)\nplt.title(\"Correlation Heatmap of Numerical Features\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHighly Correlated Features\n\nthreshold = 0.8\nhigh_corr_pairs = corr_matrix.abs().unstack().sort_values(ascending = False).drop_duplicates()\nhigh_corr_pairs = high_corr_pairs[high_corr_pairs &gt; threshold]\n\nhigh_corr_pairs\n\ncounty_fips_code    county_fips_code     1.000000\ntwo_races           one_race             1.000000\ntotal_pop           households           0.996420\nh_under6_single_m   h_6to17_single_m     0.996162\nh_under6_both_work  h_6to17_both_work    0.995615\n                                           ...   \nmfccsa              mcsa                 0.827209\nme_2018             mhi_2018             0.825270\nmfccsa              mfcc_infant          0.824414\none_race_w          one_race_b           0.824256\nemp_service         memp_service         0.805360\nLength: 88, dtype: float64"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-5---data-splitting-1-cr.",
    "href": "hw-03-manasppanse.html#task-5---data-splitting-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 5 - Data Splitting (1 Cr.)",
    "text": "Task 5 - Data Splitting (1 Cr.)\n\n# Defining Features and Target\nx = costs_df.drop(columns = ['above_state_avg'])\ny = costs_df['above_state_avg']\n\n# Dataset Split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n# Displaying Shapes of Datasets\nprint(f'x_train Shape : {x_train.shape}')\nprint(f'X_test Shape : {x_test.shape}')\nprint(f'y_train Shape : {y_train.shape}')\nprint(f'y_test Shape : {y_test.shape}')\n\nx_train Shape : (18673, 64)\nX_test Shape : (4669, 64)\ny_train Shape : (18673,)\ny_test Shape : (4669,)"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-6---model-training-1-cr.",
    "href": "hw-03-manasppanse.html#task-6---model-training-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 6 - Model Training (1 Cr.)",
    "text": "Task 6 - Model Training (1 Cr.)\n\nLogistic Regression\n\nlog_reg = LogisticRegression(max_iter = 1000)\nlog_reg.fit(x_train, y_train)\ny_pred_log_reg = log_reg.predict(x_test)\nlog_reg_acc = accuracy_score(y_test, y_pred_log_reg)\nprint(f'Logistic Regression Accuracy : {log_reg_acc:.4f}')\n\nLogistic Regression Accuracy : 0.7190\n\n\n\n\nDecision Tree Classifier\n\ntree_clf = DecisionTreeClassifier(random_state = 42)\ntree_clf.fit(x_train, y_train)\ny_pred_tree = tree_clf.predict(x_test)\ntree_acc = accuracy_score(y_test, y_pred_tree)\nprint(f'Decision Tree Accuracy : {tree_acc:.4f}')\n\nDecision Tree Accuracy : 0.9715\n\n\n\n\nRandom Forest Classifier\n\nforest_clf = RandomForestClassifier(random_state = 42)\nforest_clf.fit(x_train, y_train)\ny_pred_forest = forest_clf.predict(x_test)\nforest_acc = accuracy_score(y_test, y_pred_forest)\nprint(f'Random Forest Accuracy : {forest_acc:.4f}')\n\nRandom Forest Accuracy : 0.9822\n\n\n\n\nK - Nearest Neighbors Classifier\n\nknn_clf = KNeighborsClassifier(n_neighbors = 5)\nknn_clf.fit(x_train, y_train)\ny_pred_knn = knn_clf.predict(x_test)\nknn_acc = accuracy_score(y_test, y_pred_knn)\nprint(f'K-Nearest Neighbors Accuracy : {knn_acc:.4f}')\n\nK-Nearest Neighbors Accuracy : 0.7933"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-7---model-validation-2-cr.",
    "href": "hw-03-manasppanse.html#task-7---model-validation-2-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 7 - Model Validation (2 Cr.)",
    "text": "Task 7 - Model Validation (2 Cr.)\n\nscoring = {\n    'accuracy': 'accuracy',\n    'precision': make_scorer(precision_score),\n    'recall': make_scorer(recall_score),\n    'f1': make_scorer(f1_score),\n    'roc_auc': make_scorer(roc_auc_score)\n}\n\n# Cross-Validation Function\ndef evaluate_model(model, x, y):\n    scores = cross_validate(model, x, y, cv = 5, scoring = scoring)\n    print(\"‾\" * 30)\n    print(f\"Model : {model.__class__.__name__}\")\n    print(f\"Accuracy : {scores['test_accuracy'].mean():.4f}\")\n    print(f\"Precision : {scores['test_precision'].mean():.4f}\")\n    print(f\"Recall : {scores['test_recall'].mean():.4f}\")\n    print(f\"F1-score : {scores['test_f1'].mean():.4f}\")\n    print(f\"ROC-AUC : {scores['test_roc_auc'].mean():.4f}\")\n    print(\"_\" * 30)\n\n\nlog_reg = LogisticRegression(max_iter = 1000)\nevaluate_model(log_reg, x_train, y_train)\n\ntree_clf = DecisionTreeClassifier(random_state = 42)\nevaluate_model(tree_clf, x_train, y_train)\n\nforest_clf = RandomForestClassifier(random_state = 42)\nevaluate_model(forest_clf, x_train, y_train)\n\nknn_clf = KNeighborsClassifier(n_neighbors = 5)\nevaluate_model(knn_clf, x_train, y_train)\n\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : LogisticRegression\nAccuracy : 0.7068\nPrecision : 0.7170\nRecall : 0.4996\nF1-score : 0.5888\nROC-AUC : 0.6783\n______________________________\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : DecisionTreeClassifier\nAccuracy : 0.9504\nPrecision : 0.9429\nRecall : 0.9387\nF1-score : 0.9408\nROC-AUC : 0.9488\n______________________________\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : RandomForestClassifier\nAccuracy : 0.9765\nPrecision : 0.9678\nRecall : 0.9767\nF1-score : 0.9722\nROC-AUC : 0.9766\n______________________________\n‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\nModel : KNeighborsClassifier\nAccuracy : 0.7785\nPrecision : 0.7600\nRecall : 0.6909\nF1-score : 0.7238\nROC-AUC : 0.7664\n______________________________"
  },
  {
    "objectID": "hw-03-manasppanse.html#task-8---result-interpretation-1-cr.",
    "href": "hw-03-manasppanse.html#task-8---result-interpretation-1-cr.",
    "title": "Classification & Model Evaluation",
    "section": "Task 8 - Result Interpretation (1 Cr.)",
    "text": "Task 8 - Result Interpretation (1 Cr.)\nIt’s time to reveal the nominees for this assignment’s Oscar for Best Performing Model: LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, AND last but not the least KNeighborsClassifier.\nAnd the Oscar goes to … RandomForestClassifier."
  },
  {
    "objectID": "hw-05-manasppanse.html",
    "href": "hw-05-manasppanse.html",
    "title": "Clustering Techniques",
    "section": "",
    "text": "# Checking Python Version\n\n!python --version\n\nPython 3.12.3\n\n\n\n# Importing Necessary Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.stats import zscore\nfrom sklearn.cluster import KMeans\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import calinski_harabasz_score\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Setting Plot Style\nsns.set(style = \"white\")\n\n\n# Importing Dataset\nenergy_df = pd.read_csv(\"data/hw-05/owid-energy.csv\")"
  },
  {
    "objectID": "hw-05-manasppanse.html#task-1---exploratory-data-analysis-2-cr.",
    "href": "hw-05-manasppanse.html#task-1---exploratory-data-analysis-2-cr.",
    "title": "Clustering Techniques",
    "section": "Task 1 - Exploratory Data Analysis (2 Cr.)",
    "text": "Task 1 - Exploratory Data Analysis (2 Cr.)\n\nDistribution of primary_energy_consumption.\n\n# Checking Missing & Zero Values.\nprint(\"Missing Values :\", relevant_df['primary_energy_consumption'].isnull().sum())\nprint(\"Zero Values :\", (relevant_df['primary_energy_consumption'] == 0).sum())\n\nMissing Values : 0\nZero Values : 9894\n\n\n\n# Filtering out Zeros for meaningful visualization\nfiltered_data = relevant_df['primary_energy_consumption'][relevant_df['primary_energy_consumption'] &gt; 0]\n\n\n# Plotting\nplt.figure(figsize = (8, 6))\nsns.histplot(filtered_data, kde = True, color = \"tomato\")\nplt.title(\"Distribution of Primary Energy Consumption (Non-Zero Values)\")\nplt.xlabel(\"Primary Energy Consumption\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\nYou know I don’t mean to sound unprofessional, but above plot looks absolutely hideous. Let’s fix that …\n\n# Applying Log Transformation excluding Zero Values\nlog_consumption = np.log(filtered_data)\n\n# Plotting Log-Transformed Values\nplt.figure(figsize = (8, 6))\nsns.histplot(log_consumption, kde = True, color = \"tomato\")\nplt.title(\"Log-Transformed Distribution of Primary Energy Consumption (Non-Zero Values)\")\nplt.xlabel(\"Log of Primary Energy Consumption\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDistribution of electricity_generation\n\n# Checking for Missing and Zero Values\nprint(\"Missing Values :\", relevant_df['electricity_generation'].isnull().sum())\nprint(\"Zero Values :\", (relevant_df['electricity_generation'] == 0).sum())\n\nMissing Values : 0\nZero Values : 14964\n\n\n\n# Filtering out Zero for meaningful visualization\nfiltered_electricity_gen = relevant_df['electricity_generation'][relevant_df['electricity_generation'] &gt; 0]\n\n\n# Plotting\nplt.figure(figsize = (8, 6))\nsns.histplot(filtered_electricity_gen, kde = True, color = \"orange\")\nplt.title(\"Distribution of Electricity Generation (Non-Zero Values)\")\nplt.xlabel(\"Electricity Generation\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\nAnd again, the hideous-ness continues here … let’s fix that too !\n\n# Applying Log Transformation excluding Zero Values\nlog_electricity_gen = np.log(filtered_electricity_gen)\n\n# Plotting Log Transformed Values\nplt.figure(figsize = (8, 6))\nsns.histplot(log_electricity_gen, kde = True, color = \"orange\")\nplt.title(\"Log-Transformed Distribution of Electricity Generation (Non-Zero Values)\")\nplt.xlabel(\"Log of Electricity Generation\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDistribution of carbon_intensity_elec\n\n# Checking Missing Values\nprint(\"Missing Values :\", relevant_df['carbon_intensity_elec'].isnull().sum())\nprint(\"Zero Values :\", (relevant_df['carbon_intensity_elec'] == 0).sum())\n\nMissing Values : 16811\nZero Values : 24\n\n\n\n# Filtering Missing & Zero Values\nfiltered_carbon_intensity = relevant_df['carbon_intensity_elec'][(relevant_df['carbon_intensity_elec'] &gt; 0)]\n\n\n# Plotting\nplt.figure(figsize = (8, 6))\nsns.histplot(filtered_carbon_intensity, kde = True, color = \"slategray\")\nplt.title(\"Distribution of Carbon Intensity of Electricity\")\nplt.xlabel(\"Carbon Intensity of Electricity\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCoorelation Matrix\n\n# Identifying columns that are in both numeric_cols and relevant_df\nnumeric_cols_in_relevant_df = [col for col in numeric_cols if col in relevant_df.columns]\ncorrelation_matrix = relevant_df[numeric_cols_in_relevant_df].corr()\n\n# Plotting\nplt.figure(figsize = (8, 6))\nsns.heatmap(correlation_matrix, annot = False, cmap = \"coolwarm\", center = 0, square = True, linewidths = 0.5)\nplt.title(\"Correlation Matrix of Numerical Features\")\nplt.show()\n\n\n\n\n\n\n\n\n\nObservations from the Heatmap\n\nThere are strong correlations among different types of energy consumption and production metrics, as expected. For example, fossil fuel consumption is highly correlated with total primary energy consumption and electricity generation.\nRenewable energy consumption shows a positive correlation with low carbon consumption and electricity generation, indicating that countries with higher renewable energy use also tend to have higher overall low carbon energy usage.\nCarbon intensity of electricity has correlations with several types of energy consumption, which could inform clustering decisions based on environmental impact considerations."
  },
  {
    "objectID": "hw-05-manasppanse.html#task-2---feature-selection-and-data-preparation-2-cr.",
    "href": "hw-05-manasppanse.html#task-2---feature-selection-and-data-preparation-2-cr.",
    "title": "Clustering Techniques",
    "section": "Task 2 - Feature Selection and Data Preparation (2 Cr.)",
    "text": "Task 2 - Feature Selection and Data Preparation (2 Cr.)\n\n# Step 1: Feature Selection - Focus on a mix of Consumption, Production, and Environmental Impact\nfeatures_for_clustering = [\n    'biofuel_consumption', 'coal_consumption', 'gas_consumption', 'oil_consumption',\n    'renewables_consumption', 'nuclear_consumption', 'fossil_fuel_consumption',\n    'low_carbon_consumption', 'electricity_generation', 'primary_energy_consumption',\n    'carbon_intensity_elec'\n]\n\n\nclustering_df = energy_df[features_for_clustering]\nclustering_df = clustering_df.dropna(subset = ['carbon_intensity_elec'])\n\n# Imputing Missing Values in remaining columns using Mean Strategy\nimputer = SimpleImputer(strategy = 'mean')\nclustering_df_imputed = pd.DataFrame(imputer.fit_transform(clustering_df), columns = clustering_df.columns)\n\n\n# Performing Z-Scale Normalization\nscaler = StandardScaler()\nscaled_clustering_df = pd.DataFrame(\n    scaler.fit_transform(clustering_df_imputed), columns = clustering_df_imputed.columns\n)\n\n# Checking to see if Data is Ready for Clustering\nscaled_clustering_df.isnull().sum()\n\nbiofuel_consumption           0\ncoal_consumption              0\ngas_consumption               0\noil_consumption               0\nrenewables_consumption        0\nnuclear_consumption           0\nfossil_fuel_consumption       0\nlow_carbon_consumption        0\nelectricity_generation        0\nprimary_energy_consumption    0\ncarbon_intensity_elec         0\ndtype: int64"
  },
  {
    "objectID": "hw-05-manasppanse.html#task-3---kmeans-clustering-4-cr.",
    "href": "hw-05-manasppanse.html#task-3---kmeans-clustering-4-cr.",
    "title": "Clustering Techniques",
    "section": "Task 3 - KMeans Clustering (4 Cr.)",
    "text": "Task 3 - KMeans Clustering (4 Cr.)\n\n# Within-Cluster Sum of Squares\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(scaled_clustering_df)\n    wcss.append(kmeans.inertia_)\n\n# Ploting the Elbow Graph\nplt.figure(figsize = (8, 6))\nplt.plot(range(1, 11), wcss, marker = 'o', linestyle = '--')\nplt.title('Elbow Method for Optimal K')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Applying K-Means with Cluster Count 3\noptimal_k = 3 \nkmeans = KMeans(n_clusters = optimal_k, init = 'k-means++', random_state = 42)\nclustering_df_imputed['Cluster'] = kmeans.fit_predict(scaled_clustering_df)\n\n# Plotting the Cluster\nsns.pairplot(clustering_df_imputed, hue = 'Cluster', palette = 'rainbow', corner = True)\nplt.suptitle('KMeans Clustering Results', fontsize = 25)\nplt.show()"
  },
  {
    "objectID": "hw-05-manasppanse.html#task-4---hierarchical-clustering-2-cr.",
    "href": "hw-05-manasppanse.html#task-4---hierarchical-clustering-2-cr.",
    "title": "Clustering Techniques",
    "section": "Task 4 - Hierarchical Clustering (2 Cr.)",
    "text": "Task 4 - Hierarchical Clustering (2 Cr.)\n\n# Performing Hierarchical Clustering\nlinked = linkage(scaled_clustering_df, method = 'ward')\n\n# Plotting a Dendrogram for Cluster Heirarchy\nplt.figure(figsize = (8, 6))\ndendrogram(linked, labels=clustering_df_imputed.index, leaf_rotation = 90, leaf_font_size = 10, no_labels = True)\nplt.title('Dendrogram for Hierarchical Clustering')\nplt.xlabel('Samples')\nplt.ylabel('Euclidean Distances')\nplt.xticks([])\nplt.figtext(0.5, -0.05, 'NOTE: The sample ticks were removed to optimize performace and reduce clutter on screen.', wrap = True, horizontalalignment = 'center', fontsize = 5)\nplt.show()"
  }
]